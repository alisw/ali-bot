#!/usr/bin/env python

"""Get a list of pull requests that need building.

Requires a GITHUB_TOKEN in the environment.

This script's output is one commit to be built per line, with the following
fields separated by tab characters:

1. The pull request number where progress should be reported.
2. The commit hash to be checked out and built.
3. The type of build for this PR (i.e. whether it has been built before).
4. The name of an *.env file, without the .env suffix, of the repository that
   this commit belongs to.
"""

from __future__ import print_function

import glob
import hashlib
import os.path
import random
import shlex
import sys

from argparse import ArgumentParser
from collections import defaultdict
from alibot_helpers.github_utilities \
    import GithubCachedClient, PickledCache, github_token

CONFIG_PATH = os.path.join("ali-bot", "ci", "repo-config")
DEFAULTENV_NAME = "DEFAULTS.env"


def get_status_info(cgh, repo_name, ref, check_name):
    """Return the given ref's tested/success/reviewed statuses."""
    statuses = cgh.get("/repos/{repo_name}/commits/{ref}/statuses",
                       repo_name=repo_name, ref=ref)
    reviewed = tested = success = False

    for status in statuses:
        if check_name and status["context"] == check_name:
            reviewed = True
            tested = status["state"] in ["success", "error", "failure"]
            success = status["state"] == "success"
            break
        if status["context"] == "review" and status["state"] == "success":
            reviewed = True

    return {"tested": tested, "success": success, "reviewed": reviewed}


class RepoConfig:
    """Encapsulates a build configuration of the CI worker."""

    def __init__(self, build_config, cgh, args):
        self.cgh = cgh
        self.build_config = build_config
        self.check_name = ""
        self.repo_name = ""
        self.branch_ref = "master"
        self.trusted_users = []
        self.trusted_team = None
        self.trust_contributors = False
        self.worker_pool_size = args.worker_pool_size
        self.worker_index = args.worker_index

        # Parse .env files for this build config to initialise above variables.
        mesos, docker = args.mesos_role, args.container_name
        env_file_path = build_config + ".env"
        for envpath in [
                os.path.join(CONFIG_PATH, DEFAULTENV_NAME),
                os.path.join(CONFIG_PATH, mesos, DEFAULTENV_NAME),
                os.path.join(CONFIG_PATH, mesos, docker, DEFAULTENV_NAME),
                os.path.join(CONFIG_PATH, mesos, docker, env_file_path)]:
            if os.path.exists(envpath):
                self._parse_env_file(envpath)

        if not self.check_name or not self.repo_name:
            raise ValueError("CHECK_NAME and PR_REPO are required")

        print("repo", self.build_config,
              "CHECK_NAME=%s REPO_NAME=%s BRANCH_REF=%s" % (
                  self.check_name, self.repo_name, self.branch_ref),
              sep=": ", file=sys.stderr)

    def _parse_env_file(self, env_file_path):
        """Apply settings from env_file_path to this class."""
        with open(env_file_path) as envf:
            try:
                tokens = shlex.split(envf.read(), comments=False)
            except ValueError as err:
                # Some values are multiline, e.g. DEVEL_PKGS. This confuses
                # shlex. None of the variables we care about are multiline.
                print("shlex", err, "in file", env_file_path,
                      sep=": ", file=sys.stderr)
                return

            # Some variables are used elsewhere in the CI builder, so they
            # are defined as shell variables. Handle these here (including
            # multiple assignments on one line). Note that non-assignments
            # containing "=" (e.g. command arguments), variables valid only
            # for one command, and the like confuse this simple approach.
            for token in tokens:
                var, is_assignment, value = token.partition("=")
                if not is_assignment:
                    continue
                if var == "PR_REPO":
                    self.repo_name = value
                elif var == "PR_BRANCH":
                    self.branch_ref = value
                elif var == "CHECK_NAME":
                    self.check_name = value
                elif var == "TRUST_COLLABORATORS":
                    # Shell-style boolean: True if non-empty.
                    self.trust_contributors = bool(value)
                elif var == "TRUSTED_USERS":
                    self.trusted_users = value.split(",")
                elif var == "TRUSTED_TEAM" and value:
                    teams = self.cgh.get("/orgs/{org}/teams", org=self.org)
                    for team in teams:
                        if team["name"] == value:
                            self.trusted_team = team["id"]
                            break

    @property
    def org(self):
        """GitHub organization name that owns this repository."""
        return self.repo_name.split("/")[0]

    def should_process(self, commit_sha):
        """Decide whether this worker should handle the given PR.

        This is determined by the commit hash of the PR's HEAD.
        """
        sha = hashlib.new('sha256')
        sha.update(self.build_config.encode('utf-8'))
        sha.update(commit_sha.encode('utf-8'))
        intended_worker = int(sha.hexdigest(), 16) % self.worker_pool_size
        return intended_worker == self.worker_index

    def trust_pr(self, pull):
        """Determine whether the PR is trustworthy and can be built.

        If we specified a list of trusted users, teams or if we trust
        contributors, we need to check if this is the case for the given PR.
        Notice that some of these options will actually consume API calls, so
        you need to be careful about what you enable.
        """
        author_assoc = pull.get("author_association")
        return bool(
            # Trust org members' and repo owners' PRs.
            author_assoc in ("OWNER", "MEMBER", "COLLABORATOR") or

            # If requested, trust previous contributors' PRs.
            self.trust_contributors and author_assoc == "CONTRIBUTOR" or

            # Trust trusted users.
            pull["user"]["login"] in self.trusted_users or

            # If we trust a team and the user is a member, trust this PR.
            self.trusted_team and self.cgh.get(
                url="/teams/{team_id}/memberships/{login}",
                team_id=self.trusted_team, login=pull["user"]["login"]) or

            # If this PR has any approving reviews, trust it.
            any(review["state"] == "APPROVED"
                for review in self.cgh.get(
                        "/repos/{repo_name}/pulls/{pull_number}/reviews",
                        repo_name=self.repo_name,
                        pull_number=str(pull["number"]))))

    def process_pulls(self, show_main_branch):
        """Return pull requests we can process, with relevant attributes."""
        pulls = self.cgh.get("/repos/{repo_name}/pulls?base={base}",
                             repo_name=self.repo_name,
                             base=self.branch_ref)
        for pull in pulls:
            pull_hash, pull_number = pull["head"]["sha"], pull["number"]
            if not self.should_process(pull_hash):
                continue
            print("processing:", pull_number, end="... ", file=sys.stderr)
            sys.stderr.flush()

            item = {"number": pull_number, "sha": pull_hash, "repo": self,
                    "reviewed": False, "tested": False, "success": False}
            try:
                # If we specified a status to approve changes to tests we need
                # to retrieve all the statuses. If we specified a check name to
                # prioritize PR building, we need to retrieve all the statuses.
                item.update(get_status_info(
                    self.cgh, self.repo_name, pull_hash, self.check_name))

                if pull.get("draft") or \
                   pull.get("title", "").startswith("[WIP]"):
                    # Do not test draft PRs.
                    item["reviewed"] = False
                elif not item["reviewed"] and self.trust_pr(pull):
                    item["reviewed"] = True
            except RuntimeError as err:
                print(err, file=sys.stderr)
            else:
                print("done:\t%(number)7s@%(sha).7s revd=%(reviewed)d test="
                      "%(tested)d success=%(success)d" % item, file=sys.stderr)
                yield item

        # Also return the head of the main branch, but only if requested...
        if not show_main_branch:
            return

        branch = self.cgh.get("/repos/{repo_name}/branches/{branch_ref}",
                              repo_name=self.repo_name,
                              branch_ref=self.branch_ref)
        head_hash = branch["commit"]["sha"]
        # ...and only if we are the right worker. As the build results of the
        # master branch are uploaded, we want all commits of a master branch to
        # be built on the same worker, so the uploads don't conflict.
        if not self.should_process(self.branch_ref):
            return
        print("processing: master", end="... ", file=sys.stderr)
        sys.stderr.flush()
        item = {"number": self.branch_ref, "sha": head_hash, "repo": self}
        try:
            item.update(get_status_info(
                self.cgh, self.repo_name, head_hash, self.check_name))
        except RuntimeError as err:
            print(err, file=sys.stderr)
        else:
            # We consider main branches as always reviewed, since they are
            # already in the main repository.
            item["reviewed"] = True
            print("done:\t%(number)7s@%(sha).7s revd=%(reviewed)d test="
                  "%(tested)d success=%(success)d" % item, file=sys.stderr)
            yield item

    def grouped_pull_requests(self, show_main_branch):
        """Retrieve and group PRs for this build config."""
        reviewed = [pull for pull in self.process_pulls(show_main_branch)
                    if pull["reviewed"]]
        tested = [pull for pull in reviewed if pull["tested"]]
        return {
            "untested": [p for p in reviewed if not p["tested"]],
            "failed": [p for p in tested if not p["success"]],
            "succeeded": [p for p in tested if p["success"]],
        }

    def __repr__(self):
        return "RepoConfig(%s)" % self.build_config


def main(args):
    """Script entry point."""
    cache = PickledCache(args.github_cache_file)
    with GithubCachedClient(token=github_token(), cache=cache) as cgh:
        # Find .env files for this worker, parse them and find PRs to process
        # for each build config.
        grouped = defaultdict(list)
        for env_file in glob.glob(os.path.join(
                "ali-bot", "ci", "repo-config", args.mesos_role,
                args.container_name, "*.env")):
            env_file_name = os.path.basename(env_file)
            if env_file_name == "DEFAULTS.env":
                continue

            try:
                repo = RepoConfig(env_file_name[:-4], cgh, args)
            except ValueError as err:
                print(env_file_name, err, sep=": ", file=sys.stderr)
            else:
                # Extend PR groups with PRs from this repo.
                groups = repo.grouped_pull_requests(args.show_main_branch)
                for key, prs in groups.items():
                    grouped[key] += prs

    def print_prs(group, number=None):
        """Print N randomly chosen PRs from group on stdout."""
        prs = grouped[group]
        if number is not None and number <= len(prs):
            prs = random.sample(prs, number)
        for pull in prs:
            print(group, pull["number"], pull["sha"],
                  pull["repo"].build_config, sep="\t")

    if grouped["untested"]:
        # If there are untested PRs waiting, build all of them first.
        print_prs("untested")
    elif grouped["failed"] and random.random() < 0.7:
        # Rebuild a failed PR, but sometimes fall through and rebuild a
        # successful one instead. This is so a single red PR can't stop all
        # green PRs from being rebuilt occasionally.
        print_prs("failed", 1)
    elif grouped["succeeded"]:
        print_prs("succeeded", 1)


def parse_args():
    """Parse command-line arguments."""
    parser = ArgumentParser(description=__doc__, epilog="""\
    Some options are required only when the corresponding environment variable
    is not set. In case both are given, the command-line option overrides the
    environment variable.""")

    def add_env_arg(short_name, long_name, env_var, vtype=str, **kwargs):
        """Add an argument that falls back to an environment variable."""
        if "help" in kwargs:
            kwargs["help"] += (
                " (required if %(var)s is empty; default %(var)s=%(value)s)"
                % {"var": env_var, "value": os.environ.get(env_var, "")})
        # Ignore empty values from the environment!
        if os.environ.get(env_var):
            env_value = os.environ[env_var]
            try:
                typed_env_value = vtype(env_value)
            except ValueError:
                # Fall through if the value is of the incorrect type. If we
                # raised an error here, command-line options couldn't override
                # invalid values from the environment. Instead, falling through
                # makes the cmd-line option required, which is what we want.
                pass
            else:
                parser.add_argument(
                    short_name, long_name, required=False, type=vtype,
                    default=typed_env_value, **kwargs)
                return
        parser.add_argument(
            short_name, long_name, required=True, type=vtype, **kwargs)

    parser.add_argument("-m", "--show-main-branch", action="store_true",
                        help="Also show refs for the main branch")

    parser.add_argument("-f", "--github-cache-file",
                        default=os.path.expanduser(
                            "~/.cached_github_client_cache"),
                        help=("Where to cache GitHub API responses "
                              "(default %(default)s)"))

    add_env_arg("-i", "--worker-index", "WORKER_INDEX", vtype=int,
                help="Index for the current worker")

    add_env_arg("-n", "--worker-pool-size", "WORKERS_POOL_SIZE", vtype=int,
                help="Total number of workers")

    add_env_arg("-r", "--mesos-role", "MESOS_ROLE",
                help="Mesos role of the current worker")

    add_env_arg("-c", "--container-name", "CUR_CONTAINER",
                help="Short name of the container we're running in, e.g. slc8")

    return parser.parse_args()


if __name__ == "__main__":
    main(parse_args())
