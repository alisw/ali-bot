#!/usr/bin/env python
from __future__ import print_function

from argparse import ArgumentParser, Namespace
try:
    from commands import getstatusoutput
except ImportError:
    from subprocess import getstatusoutput
from glob import glob
from os.path import dirname, join, expanduser
import re
import os
import sys
import datetime

from alibot_helpers.github_utilities import calculateMessageHash, github_token
from alibot_helpers.github_utilities import setGithubStatus, parseGithubRef
from alibot_helpers.github_utilities import GithubCachedClient, PickledCache
from alibot_helpers.utilities import to_unicode

# Allow uploading logs to S3
try:
    from boto3.s3.transfer import S3Transfer
    import boto3
except ImportError:
    pass


def parse_args():
    parser = ArgumentParser()
    parser.add_argument("--work-dir", "-w", default="sw", dest="workDir")

    parser.add_argument("--default", default="release")

    parser.add_argument("--devel-prefix", "-z",
                        dest="develPrefix",
                        default="")

    parser.add_argument("--pr",
                        required=True,
                        help=("Pull request which was checked in "
                              "<org>/<project>#<nr>@ref format"))

    parser.add_argument("--no-comments",
                        action="store_true",
                        dest="noComments",
                        default=False,
                        help="Use Details button, do not post a comment")

    status_gp = parser.add_mutually_exclusive_group()

    status_gp.add_argument("--success",
                           action="store_true",
                           dest="success",
                           default=False,
                           help="Signal a green status, not error")

    status_gp.add_argument("--pending",
                           action="store_true",
                           help="Signal only that the build has started")

    parser.add_argument("--status", "-s",
                        required=True,
                        help="Check which had the error")

    parser.add_argument("--dry-run", "-n",
                        action="store_true",
                        default=False,
                        help="Do not actually comment")

    parser.add_argument("--limit", "-l",
                        default=50,
                        help="Max number of lines from the report")

    parser.add_argument("--message", "-m",
                        dest="message",
                        help="Message to be posted")

    parser.add_argument("--logs-dest",
                        dest="logsDest",
                        default="rsync://repo.marathon.mesos/store/logs",
                        help="Destination store for logs. Either rsync://<rsync server enpoint> or s3://<bucket>.<server>")

    parser.add_argument("--log-url",
                        dest="logsUrl",
                        default="https://ali-ci.cern.ch/repo/logs",
                        help="Destination path for logs")

    parser.add_argument("--github-cache-file", default=expanduser("~/.cached-commits"),
                        help="Where to cache GitHub API responses (default %(default)s)")

    parser.add_argument("--debug", "-d",
                        action="store_true",
                        default=False,
                        help="Turn on debug output")

    args = parser.parse_args()
    if "#" not in args.pr:
        parser.error("You need to specify a pull request")
    if "@" not in args.pr:
        parser.error("You need to specify a commit this error refers to")
    return args


def run_cmd(command, output_only_if_error=False):
    print(command, file=sys.stderr)
    err, out = getstatusoutput(command)
    if not output_only_if_error or err:
        print(out, file=sys.stderr)


class Logs(object):
    def __init__(self, args, is_branch):
        self.work_dir = args.workDir
        self.develPrefix = args.develPrefix
        self.limit = args.limit
        self.norm_status = re.sub('[^a-zA-Z0-9_-]', '_', args.status)
        self.full_log = self.constructFullLogName(args.pr)
        self.is_branch = is_branch
        self.full_log_latest = self.constructFullLogName(args.pr, latest=True)
        self.pretty_log = self.constructFullLogName(args.pr, pretty=True)
        self.dest = args.logsDest
        self.url = join(args.logsUrl, self.pretty_log)
        self.log_url = join(args.logsUrl, self.full_log)
        self.build_successful = args.success

    def parse(self):
        self.find()
        self.grep()
        self.cat(self.full_log)
        self.generate_pretty_log()
        if self.is_branch:
            self.cat(self.full_log_latest, no_delete=True)
        if self.dest.startswith("rsync:"):
            self.rsync(self.dest)
        elif self.dest.startswith("s3"):
            self.s3Upload(self.dest)
        else:
            print("Unknown destination url %s" % self.dest)

    def constructFullLogName(self, pr, latest=False, pretty=False):
        # file to which we cat all the individual logs
        pr = parse_pr(pr)
        return join(pr.repo_name, pr.id, "latest" if latest else pr.commit, self.norm_status,
                    "pretty.html" if pretty else "fullLog.txt")

    def find(self):
        # Python's glob * matches at least one char
        search_paths = [join(self.work_dir, "BUILD/*latest*/log"),
                        join(self.work_dir, "BUILD/*latest/log")]
        print("Searching all logs matching: %s" % ", ".join(search_paths), file=sys.stderr)
        globbed = []
        for sp in search_paths:
            globbed.extend(glob(sp))

        suffix = ("latest" + "-" + self.develPrefix).strip("-")
        logs = {x for x in globbed if dirname(x).endswith(suffix)}

        print("Found:\n%s" % "\n".join(logs), file=sys.stderr)
        self.logs = logs

    def grep(self):
        """Grep for errors in the build logs, or, if none are found,
        return the last N lines where N is the limit argument.

        Also extract messages from failed unit tests and o2checkcode.
        """
        error_log_lines = []
        for log in self.logs:
            err, out = getstatusoutput("grep -he ': error:' -A 3 -B 3 -- %s "
                                       "|| tail -n %s %s" % (log, self.limit, log))
            if err:
                print("Error while parsing logs", out, sep="\n", file=sys.stderr)
                continue
            error_log_lines.append(log)
            error_log_lines.extend(out.split("\n"))
        self.error_log = "\n".join(error_log_lines[:self.limit]).strip(" \n\t")

        def chunk_command_output_by_log(command):
            blocks = []
            for log in self.logs:
                err, out = getstatusoutput(command % log)
                if err:
                    print("Error while parsing logs", out, sep="\n", file=sys.stderr)
                    continue
                if out:
                    blocks.append(log + "\n" + out)
            return "\n\n\n".join(blocks).strip(" \n\t")

        self.errors_only_log = chunk_command_output_by_log(
            "grep -he ': error:' -e ': warning:' -A 3 -B 3 -- %s")
        self.o2checkcode_messages = chunk_command_output_by_log(
            "sed -n '/=== List of errors found ===/,$p' %s")
        self.failed_unit_tests = chunk_command_output_by_log(
            r"grep -he 'Test *#[0-9]*: .*\*\*\*Failed' -e '%% tests passed' -- %s")
        self.compiler_killed = chunk_command_output_by_log(
            "grep -he 'fatal error: Killed signal terminated program' -- %s")

    def generate_pretty_log(self):
        '''Extract error messages from logs.

        The errors are presented on a user-friendly HTML page, to be uploaded
        alongside the full message log.
        '''
        def htmlescape(string):
            return string.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')

        def display(string):
            return 'block' if string else 'none'

        with open(join('copy-logs', self.pretty_log), 'w') as f:
            f.write(PRETTY_LOG_TEMPLATE % {
                'status': 'succeeded' if self.build_successful else 'failed',
                'log_url': self.log_url,
                'o2checkcode': htmlescape(self.o2checkcode_messages),
                'o2c_display': display(self.o2checkcode_messages),
                'unittests': htmlescape(self.failed_unit_tests),
                'unit_display': display(self.failed_unit_tests),
                'errors': htmlescape(self.errors_only_log),
                'err_display': display(self.errors_only_log),
                'killed_display': display(self.compiler_killed),
                'noerrors_display': display(not any((
                    self.o2checkcode_messages, self.failed_unit_tests,
                    self.errors_only_log, self.compiler_killed))),
            })

    def cat(self, tgtFile, no_delete=False):
        run_cmd("%s && mkdir -p `dirname copy-logs/%s`" % ("true" if no_delete else "rm -rf copy-logs", tgtFile),
                output_only_if_error=True)

        run_cmd('echo "Builing on: $(hostname) at $(date +%Y-%m-%d-%H:%M:%S)" >> copy-logs/' + tgtFile)

        if len(self.logs):
            run_cmd("echo 'The following files are present in the log:' >> copy-logs/%s" % tgtFile)
        else:
            run_cmd("echo 'No logs found. Please check actual builders log in aurora.\n"
                    "See http://alisw.github.io/infrastructure-pr-testing for more instructions.' >> copy-logs/%s" % tgtFile)

        for log in self.logs:
            run_cmd("echo '- %s' >> copy-logs/%s" % (log, tgtFile))

        for log in self.logs:
            run_cmd(("{ echo '## Begin %(log)s'; cat %(log)s; echo '## End %(log)s'; }"
                     " >> copy-logs/%(tgtFile)s") % {'log': log, 'tgtFile': tgtFile})

    def rsync(self, dest):
        err, out = getstatusoutput("cd copy-logs && rsync -av ./ %s" % dest)
        if err:
            print("Error while copying logs to store.", file=sys.stderr)
            print(out, file=sys.stderr)

    def s3Upload(self, dest):
        m = re.compile("^s3://([^.]+)[.](.*)").match(dest)
        bucket_name = m.group(1)
        server = m.group(2)
        s3_client = boto3.client('s3',
                                 aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
                                 aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
                                 endpoint_url="https://%s" % (server))
        s3_client.list_buckets()
        transfer = S3Transfer(s3_client)
        matches = []
        for root, dirnames, filenames in os.walk('copy-logs'):
            for filename in filenames:
                matches.append(os.path.join(root, filename))
        for src in matches:
            try:
                dst = re.compile('^copy-logs/').sub('', src)
                transfer.upload_file(src, bucket_name, dst, extra_args={
                    'ContentType': 'text/html' if src.endswith('.html') else 'text/plain',
                    'ContentDisposition': 'inline'})
            except Exception as e:
                print("Failed to upload %s to %s in bucket %s.%s" % (src, dst, bucket_name, server))


def get_pending_namespace(args):
    '''Return a Namespace to set status to pending with an appropriate message.'''
    return Namespace(commit=args.pr,
                     status=args.status + "/pending",
                     message=args.message,
                     url="")


def handle_branch(cgh, pr, logs, args):
    ns = get_pending_namespace(args) if args.pending else \
        Namespace(commit=args.pr,
                  status=args.status + "/error",
                  message="",
                  url="")
    setGithubStatus(cgh, ns)
    sys.exit(0)


def handle_pr_id(cgh, pr, logs, args):
    commit = cgh.get("/repos/{repo_name}/commits/{ref}",
                     repo_name=pr.repo_name,
                     ref=pr.commit)
    sha = commit["sha"]

    if not args.pending:
        message = "Error while checking %s for %s at %s:\n" % (args.status, sha, datetime.datetime.now().strftime("%Y-%m-%d %H:%M"))
        if args.message:
            message += args.message
        else:
            message += "```\n%s\n```\nFull log [here](%s).\n" % (to_unicode(logs.error_log), to_unicode(logs.url))

    if args.dry_run:
        # commit does not exist...
        print("Will annotate %s" % commit["sha"])
        print(message)
        sys.exit(0)

    # Set status
    ns = get_pending_namespace(args) if args.pending else \
        Namespace(commit=args.pr,
                  status=args.status + ("/success" if args.success else "/error"),
                  message="",
                  url=logs.url)
    setGithubStatus(cgh, ns)

    # Comment if appropriate
    if args.noComments or args.success or args.pending:
        return

    prIssueComments = cgh.get("/repos/{repo_name}/issues/{pr_id}/comments",
                              repo_name=pr.repo_name,
                              pr_id=pr.id)

    messageHash = calculateMessageHash(message)
    for comment in prIssueComments:
        if comment["body"].startswith("Error while checking %s for %s" % (args.status, sha)):
            if calculateMessageHash(comment["body"]) != messageHash:
                print("Comment was different. Updating", file=sys.stderr)
                cgh.patch(
                    "/repos/{repo_name}/issues/comments/{commentID}",
                    {"body": message},
                    repo_name=pr.repo_name,
                    commentID=comment["id"]
                )
                sys.exit(0)

            print("Found same comment for the same commit", file=sys.stderr)
            sys.exit(0)


    cgh.post(
        "repos/{repo_name}/issues/{pr_id}/comments",
        {"body": message},
        repo_name=pr.repo_name,
        pr_id=pr.id
    )


def parse_pr(pr):
    repo_name, pr_id, pr_commit = parseGithubRef(pr)
    return Namespace(repo_name=repo_name,
                     id=pr_id,
                     commit=pr_commit)


def main():
    args = parse_args()
    pr = parse_pr(args.pr)
    logs = Logs(args, is_branch=not pr.id.isdigit())
    if not args.message and not args.pending:
        logs.parse()

    cache = PickledCache(args.github_cache_file)
    with GithubCachedClient(token=github_token(), cache=cache) as cgh:
        # If the branch is not a PR, we should look for open issues
        # for the branch. This should really folded as a special case
        # of the PR case.
        func = handle_branch if not pr.id.isdigit() else handle_pr_id
        func(cgh, pr, logs, args)

    cgh.printStats()


PRETTY_LOG_TEMPLATE = '''\
<!DOCTYPE html>
<head>
  <title>Build results</title>
  <style>
   body { border-width: 0.5rem; border-style: solid; margin: 0; padding: 1rem; min-height: 100vh; box-sizing: border-box; }
   pre { overflow-x: auto; }
   .succeeded { border-color: #1b5e20; }
   .succeeded h1 { color: #1b5e20; }
   .failed { border-color: #b71c1c; }
   .failed h1 { color: #b71c1c; }
   #noerrors, #noerrors-toc { display: %(noerrors_display)s; }
   #compiler-killed, #compiler-killed-toc { display: %(killed_display)s; }
   #o2checkcode, #o2checkcode-toc { display: %(o2c_display)s; }
   #tests, #tests-toc { display: %(unit_display)s; }
   #errors, #errors-toc { display: %(err_display)s; }
  </style>
</head>
<body class="%(status)s">
  <h1>The build %(status)s</h1>
  <p>The code that finds and extracts error messages may have missed some.</p>
  <p>Check the <a href="%(log_url)s">full build log</a> if you suspect the build failed for reasons not listed below.</p>
  <h3>Table of contents</h3>
  <p><nav><ol>
    <li id="noerrors-toc"><a href="#noerrors">No errors found</a></li>
    <li id="compiler-killed-toc"><a href="#compiler-killed">Error: the compiler process was killed</a></li>
    <li id="o2checkcode-toc"><a href="#o2checkcode"><code>o2checkcode</code> results</a></li>
    <li id="tests-toc"><a href="#tests">Unit test results</a></li>
    <li id="errors-toc"><a href="#errors">Error messages</a></li>
  </ol></nav></p>
  <section id="noerrors">
    <h2>No errors found</h2>
    <p>Check the <a href="%(log_url)s">full build log</a> to see all compilation messages.</p>
  </section>
  <section id="compiler-killed">
    <h2>Error: the compiler process was killed</h2>
    <p>This can happen in case of memory pressure. A later automatic build may solve this.</p>
  </section>
  <section id="o2checkcode">
    <h2><code>o2checkcode</code> results</h2>
    <p><pre><code>%(o2checkcode)s</code></pre></p>
  </section>
  <section id="tests">
    <h2>Unit test results</h2>
    <p><pre><code>%(unittests)s</code></pre></p>
  </section>
  <section id="errors">
    <h2>Error and warning messages</h2>
    <p>Note that the following list may include false positives! Check the sections above first.</p>
    <p><pre><code>%(errors)s</code></pre></p>
  </section>
</body>
'''

if __name__ == "__main__":
    main()
