#!/usr/bin/env python
from __future__ import print_function

from argparse import ArgumentParser, Namespace
try:
    from commands import getstatusoutput
except ImportError:
    from subprocess import getstatusoutput
from glob import glob
from os.path import dirname, join, expanduser
import re
import os
import platform
import shutil
import sys
import datetime

from alibot_helpers.github_utilities import calculateMessageHash, github_token
from alibot_helpers.github_utilities import setGithubStatus, parseGithubRef
from alibot_helpers.github_utilities import GithubCachedClient, PickledCache
from alibot_helpers.utilities import to_unicode

# Allow uploading logs to S3
try:
    from boto3.s3.transfer import S3Transfer
    import boto3
except ImportError:
    pass


def parse_args():
    parser = ArgumentParser()
    parser.add_argument("--work-dir", "-w", default="sw", dest="workDir")

    parser.add_argument("--default", default="release")

    parser.add_argument("--devel-prefix", "-z",
                        dest="develPrefix",
                        default="")

    parser.add_argument("--pr",
                        required=True,
                        help=("Pull request which was checked in "
                              "<org>/<project>#<nr>@ref format"))

    parser.add_argument("--no-comments",
                        action="store_true",
                        dest="noComments",
                        default=False,
                        help="Use Details button, do not post a comment")

    status_gp = parser.add_mutually_exclusive_group()

    status_gp.add_argument("--success",
                           action="store_true",
                           dest="success",
                           default=False,
                           help="Signal a green status, not error")

    status_gp.add_argument("--pending",
                           action="store_true",
                           help="Signal only that the build has started")

    parser.add_argument("--status", "-s",
                        required=True,
                        help="Check which had the error")

    parser.add_argument("--dry-run", "-n",
                        action="store_true",
                        default=False,
                        help="Do not actually comment")

    parser.add_argument("--limit", "-l",
                        default=50,
                        help="Max number of lines from the report")

    parser.add_argument("--message", "-m",
                        dest="message",
                        help="Message to be posted")

    parser.add_argument("--logs-dest",
                        dest="logsDest",
                        default="rsync://repo.marathon.mesos/store/logs",
                        help="Destination store for logs. Either rsync://<rsync server enpoint> or s3://<bucket>.<server>")

    parser.add_argument("--log-url",
                        dest="logsUrl",
                        default="https://ali-ci.cern.ch/repo/logs",
                        help="Destination path for logs")

    parser.add_argument("--github-cache-file", default=expanduser("~/.cached-commits"),
                        help="Where to cache GitHub API responses (default %(default)s)")

    parser.add_argument("--debug", "-d",
                        action="store_true",
                        default=False,
                        help="Turn on debug output")

    args = parser.parse_args()
    if "#" not in args.pr:
        parser.error("You need to specify a pull request")
    if "@" not in args.pr:
        parser.error("You need to specify a commit this error refers to")
    return args


class Logs(object):
    def __init__(self, args, is_branch):
        self.work_dir = args.workDir
        self.develPrefix = args.develPrefix
        self.limit = args.limit
        self.norm_status = re.sub('[^a-zA-Z0-9_-]', '_', args.status)
        self.full_log = self.constructFullLogName(args.pr)
        self.is_branch = is_branch
        self.full_log_latest = self.constructFullLogName(args.pr, latest=True)
        self.pretty_log = self.constructFullLogName(args.pr, pretty=True)
        self.dest = args.logsDest
        self.url = join(args.logsUrl, self.pretty_log)
        self.log_url = join(args.logsUrl, self.full_log)
        self.build_successful = args.success
        self.pr_built = parse_pr(args.pr)

    def parse(self):
        self.find()
        self.grep()
        self.cat(self.full_log)
        self.generate_pretty_log()
        if self.is_branch:
            self.cat(self.full_log_latest, no_delete=True)
        if self.dest.startswith("rsync:"):
            self.rsync(self.dest)
        elif self.dest.startswith("s3"):
            self.s3Upload(self.dest)
        else:
            print("Unknown destination url %s" % self.dest)

    def constructFullLogName(self, pr, latest=False, pretty=False):
        # file to which we cat all the individual logs
        pr = parse_pr(pr)
        return join(pr.repo_name, pr.id, "latest" if latest else pr.commit, self.norm_status,
                    "pretty.html" if pretty else "fullLog.txt")

    def find(self):
        search_path = join(self.work_dir, "BUILD/*latest*/log")
        print("Searching all logs matching: %s" % search_path, file=sys.stderr)
        suffix = ("latest-" + self.develPrefix).rstrip("-")
        self.logs = {x for x in glob(search_path)
                     if dirname(x).endswith(suffix)}
        print("Found:\n%s" % "\n".join(self.logs), file=sys.stderr)

    def grep(self):
        """Grep for errors in the build logs, or, if none are found,
        return the last N lines where N is the limit argument.

        Also extract messages from failed unit tests and o2checkcode.
        """
        error_log_lines = []
        for log in self.logs:
            err, out = getstatusoutput("grep -he ': error:' -A 3 -B 3 -- %s "
                                       "|| tail -n %s %s" % (log, self.limit, log))
            if err:
                print("Error while parsing logs", out, sep="\n", file=sys.stderr)
                continue
            error_log_lines.append(log)
            error_log_lines.extend(out.split("\n"))
        self.error_log = "\n".join(error_log_lines[:self.limit]).strip(" \n\t")

        def chunk_command_output_by_log(command, ignore_log_files=()):
            ignore_log_files = [join(self.work_dir, 'BUILD', log, 'log')
                                for log in ignore_log_files]
            blocks = []
            for log in self.logs:
                if log in ignore_log_files:
                    continue
                err, out = getstatusoutput(command % log)
                if err:
                    # Most likely caused by grep not finding the search term.
                    print("error while parsing logs:", out, file=sys.stderr)
                    continue
                if out:
                    blocks.append(log + "\n" + out)
            return "\n\n\n".join(blocks).strip(" \n\t")

        self.errors_only_log = chunk_command_output_by_log(
            "grep -he ': internal compiler error:' -e ': error:' -e ': warning:' -A 3 -B 3 -- %s",
            # Errors from this log are reported in o2checkcode_messages
            # already, so don't report them twice. This log also contains false
            # positives, so o2checkcode_messages is better.
            ignore_log_files=['o2checkcode-latest'])
        self.o2checkcode_messages = chunk_command_output_by_log(
            "sed -n '/=== List of errors found ===/,$p' %s")
        self.failed_unit_tests = chunk_command_output_by_log(
            r"grep -he 'Test *#[0-9]*: .*\*\*\*Failed' -e '%% tests passed' -- %s")
        self.compiler_killed = chunk_command_output_by_log(
            "grep -he 'fatal error: Killed signal terminated program' -- %s")
        self.cmake_errors = chunk_command_output_by_log(
            "sed -En '/^CMake (Error|Warning)/,/^$/p' %s")

    def generate_pretty_log(self):
        '''Extract error messages from logs.

        The errors are presented on a user-friendly HTML page, to be uploaded
        alongside the full message log.
        '''
        def htmlescape(string):
            return string.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')

        def display(string):
            return 'block' if string else 'none'

        with open(join('copy-logs', self.pretty_log), 'w') as f:
            f.write(PRETTY_LOG_TEMPLATE % {
                'status': 'succeeded' if self.build_successful else 'failed',
                'log_url': self.log_url,
                'o2checkcode': htmlescape(self.o2checkcode_messages),
                'o2c_display': display(self.o2checkcode_messages),
                'unittests': htmlescape(self.failed_unit_tests),
                'unit_display': display(self.failed_unit_tests),
                'errors': htmlescape(self.errors_only_log),
                'err_display': display(self.errors_only_log),
                'cmake': htmlescape(self.cmake_errors),
                'cmake_display': display(self.cmake_errors),
                'killed_display': display(self.compiler_killed),
                'noerrors_display': display(not any((
                    self.o2checkcode_messages, self.failed_unit_tests,
                    self.errors_only_log, self.compiler_killed,
                    self.cmake_errors))),
                'hostname': htmlescape(platform.node()),
                'finish_time': datetime.datetime.utcnow().strftime('%a %-d %b %Y, %H:%M:%S UTC'),
                'repo': self.pr_built.repo_name,
                'pr_id': self.pr_built.id,
                'commit': self.pr_built.commit,
                'login_cmd': re.sub(
                    '^(?:thermos-)?([a-z]*)-([a-z]*)-([a-z0-9_-]*)-([0-9]*)(?:-[0-9a-f]*){5}$',
                    r'aurora task ssh -l root build/\1/\2/\3/\4',
                    os.getenv('MESOS_EXECUTOR_ID', '(no Mesos ID)')),
            })

    def cat(self, target_file, no_delete=False):
        if not no_delete:
            def print_delete_error(func, path, exc):
                print(func.__name__, "could not delete", path,
                      sep=": ", file=sys.stderr)
            shutil.rmtree("copy-logs", onerror=print_delete_error)
        try:
            os.makedirs(dirname(join("copy-logs", target_file)))
        except OSError as err:
            # Directory already exists. OSError is raised on python2 and
            # FileExistsError on python3; the latter is a subclass of OSError,
            # so this clause handles both.
            print("cannot create target dir:", err, file=sys.stderr)

        with open(join("copy-logs", target_file), "w") as logf:
            print("Finished building on", platform.node(), "at",
                  datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S"),
                  file=logf)
            print("Built commit", self.pr_built.commit, "from",
                  "https://github.com/%s/pull/%s" % (self.pr_built.repo_name,
                                                     self.pr_built.id),
                  file=logf)
            if "MESOS_EXECUTOR_ID" in os.environ:
                mesos_id = os.environ["MESOS_EXECUTOR_ID"]
                print("To log into the build machine, use:\n",
                      "   aurora task ssh -l root",
                      re.sub("^(?:thermos-)?([a-z]*)-([a-z]*)-([a-z0-9_-]*)-([0-9]*)(?:-[0-9a-f]*){5}$",
                             r"build/\1/\2/\3/\4", mesos_id),
                      file=logf)
            if self.logs:
                print("The following files are present in the log:", file=logf)
                for log in self.logs:
                    print("-", log, file=logf)
            else:
                print("No logs found. Please check the aurora log.",
                      "See http://alisw.github.io/infrastructure-pr-testing for more instructions.",
                      sep="\n", file=logf)
            for log in self.logs:
                print("## Begin", log, file=logf)
                with open(log) as sublogf:
                    logf.writelines(sublogf)
                print("## End", log, file=logf)

    def rsync(self, dest):
        err, out = getstatusoutput("cd copy-logs && rsync -av ./ %s" % dest)
        if err:
            print("Error while copying logs to store.", file=sys.stderr)
            print(out, file=sys.stderr)

    def s3Upload(self, dest):
        m = re.compile("^s3://([^.]+)[.](.*)").match(dest)
        bucket_name = m.group(1)
        server = m.group(2)
        s3_client = boto3.client('s3',
                                 aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
                                 aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
                                 endpoint_url="https://%s" % (server))
        s3_client.list_buckets()
        transfer = S3Transfer(s3_client)
        matches = []
        for root, dirnames, filenames in os.walk('copy-logs'):
            for filename in filenames:
                matches.append(join(root, filename))
        for src in matches:
            try:
                dst = re.compile('^copy-logs/').sub('', src)
                transfer.upload_file(src, bucket_name, dst, extra_args={
                    'ContentType': 'text/html' if src.endswith('.html') else 'text/plain',
                    'ContentDisposition': 'inline'})
            except Exception as e:
                print("Failed to upload %s to %s in bucket %s.%s" % (src, dst, bucket_name, server))


def get_pending_namespace(args):
    '''Return a Namespace to set status to pending with an appropriate message.'''
    return Namespace(commit=args.pr,
                     status=args.status + "/pending",
                     message=args.message,
                     url="")


def handle_branch(cgh, pr, logs, args):
    ns = get_pending_namespace(args) if args.pending else \
        Namespace(commit=args.pr,
                  status=args.status + "/error",
                  message="",
                  url="")
    setGithubStatus(cgh, ns)
    sys.exit(0)


def handle_pr_id(cgh, pr, logs, args):
    commit = cgh.get("/repos/{repo_name}/commits/{ref}",
                     repo_name=pr.repo_name,
                     ref=pr.commit)
    sha = commit["sha"]

    if not args.pending:
        message = "Error while checking %s for %s at %s:\n" % (args.status, sha, datetime.datetime.now().strftime("%Y-%m-%d %H:%M"))
        if args.message:
            message += args.message
        else:
            message += "```\n%s\n```\nFull log [here](%s).\n" % (to_unicode(logs.error_log), to_unicode(logs.url))

    if args.dry_run:
        # commit does not exist...
        print("Will annotate %s" % commit["sha"])
        print(message)
        sys.exit(0)

    # Set status
    ns = get_pending_namespace(args) if args.pending else \
        Namespace(commit=args.pr,
                  status=args.status + ("/success" if args.success else "/error"),
                  message="",
                  url=logs.url)
    setGithubStatus(cgh, ns)

    # Comment if appropriate
    if args.noComments or args.success or args.pending:
        return

    prIssueComments = cgh.get("/repos/{repo_name}/issues/{pr_id}/comments",
                              repo_name=pr.repo_name,
                              pr_id=pr.id)

    messageHash = calculateMessageHash(message)
    for comment in prIssueComments:
        if comment["body"].startswith("Error while checking %s for %s" % (args.status, sha)):
            if calculateMessageHash(comment["body"]) != messageHash:
                print("Comment was different. Updating", file=sys.stderr)
                cgh.patch(
                    "/repos/{repo_name}/issues/comments/{commentID}",
                    {"body": message},
                    repo_name=pr.repo_name,
                    commentID=comment["id"]
                )
                sys.exit(0)

            print("Found same comment for the same commit", file=sys.stderr)
            sys.exit(0)


    cgh.post(
        "repos/{repo_name}/issues/{pr_id}/comments",
        {"body": message},
        repo_name=pr.repo_name,
        pr_id=pr.id
    )


def parse_pr(pr):
    repo_name, pr_id, pr_commit = parseGithubRef(pr)
    return Namespace(repo_name=repo_name,
                     id=pr_id,
                     commit=pr_commit)


def main():
    args = parse_args()
    pr = parse_pr(args.pr)
    logs = Logs(args, is_branch=not pr.id.isdigit())
    if not args.message and not args.pending:
        logs.parse()

    cache = PickledCache(args.github_cache_file)
    with GithubCachedClient(token=github_token(), cache=cache) as cgh:
        # If the branch is not a PR, we should look for open issues
        # for the branch. This should really folded as a special case
        # of the PR case.
        func = handle_branch if not pr.id.isdigit() else handle_pr_id
        func(cgh, pr, logs, args)

    cgh.printStats()


PRETTY_LOG_TEMPLATE = '''\
<!DOCTYPE html>
<head>
  <title>Build results</title>
  <style>
   body { border-width: 0.5rem; border-style: solid; margin: 0; padding: 1rem; min-height: 100vh; box-sizing: border-box; }
   pre { overflow-x: auto; }
   .succeeded { border-color: #1b5e20; }
   .succeeded h1 { color: #1b5e20; }
   .failed { border-color: #b71c1c; }
   .failed h1 { color: #b71c1c; }
   #noerrors, #noerrors-toc { display: %(noerrors_display)s; }
   #compiler-killed, #compiler-killed-toc { display: %(killed_display)s; }
   #o2checkcode, #o2checkcode-toc { display: %(o2c_display)s; }
   #tests, #tests-toc { display: %(unit_display)s; }
   #errors, #errors-toc { display: %(err_display)s; }
   #cmake, #cmake-toc { display: %(cmake_display)s; }
  </style>
</head>
<body class="%(status)s">
  <h1>The build %(status)s</h1>
  <table>
    <tr><th>Build host</th><td>%(hostname)s</td></tr>
    <tr><th>Finished at</th><td>%(finish_time)s</td></tr>
    <tr><th>Built PR</th>
        <td><a href="https://github.com/%(repo)s/pull/%(pr_id)s">%(repo)s#%(pr_id)s</a></td></tr>
    <tr><th>Built commit</th>
        <td><a href="https://github.com/%(repo)s/pull/%(pr_id)s/commits/%(commit)s"><code>%(commit)s</code></a></td></tr>
  </table>
  <p>Use the following command to log in to the build machine:
     <pre><code>%(login_cmd)s</code></pre></p>
  <p>The code that finds and extracts error messages may have missed some.</p>
  <p>Check the <a href="%(log_url)s">full build log</a> if you suspect the build failed for reasons not listed below.</p>
  <h3>Table of contents</h3>
  <p><nav><ol>
    <li id="noerrors-toc"><a href="#noerrors">No errors found</a></li>
    <li id="compiler-killed-toc"><a href="#compiler-killed">Error: the compiler process was killed</a></li>
    <li id="cmake-toc"><a href="#cmake">CMake errors and warnings</a></li>
    <li id="o2checkcode-toc"><a href="#o2checkcode"><code>o2checkcode</code> results</a></li>
    <li id="tests-toc"><a href="#tests">Unit test results</a></li>
    <li id="errors-toc"><a href="#errors">Error messages</a></li>
  </ol></nav></p>
  <section id="noerrors">
    <h2>No errors found</h2>
    <p>Check the <a href="%(log_url)s">full build log</a> to see all compilation messages.</p>
  </section>
  <section id="cmake">
    <h2>CMake errors and warnings</h2>
    <p><pre><code>%(cmake)s</code></pre></p>
  </section>
  <section id="compiler-killed">
    <h2>Error: the compiler process was killed</h2>
    <p>This can happen in case of memory pressure. A later automatic build may solve this.</p>
  </section>
  <section id="o2checkcode">
    <h2><code>o2checkcode</code> results</h2>
    <p><pre><code>%(o2checkcode)s</code></pre></p>
  </section>
  <section id="tests">
    <h2>Unit test results</h2>
    <p><pre><code>%(unittests)s</code></pre></p>
  </section>
  <section id="errors">
    <h2>Error and warning messages</h2>
    <p>Note that the following list may include false positives! Check the sections above first.</p>
    <p><pre><code>%(errors)s</code></pre></p>
  </section>
</body>
'''

if __name__ == "__main__":
    main()
