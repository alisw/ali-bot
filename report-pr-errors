#!/usr/bin/env python
from __future__ import print_function

from argparse import ArgumentParser, Namespace
try:
    from commands import getstatusoutput
except ImportError:
    from subprocess import getstatusoutput
from collections import deque
from fnmatch import fnmatch
from glob import glob
from os.path import dirname, join, expanduser
import re
import os
import platform
import shutil
import sys
import datetime

from alibot_helpers.github_utilities import calculateMessageHash, github_token
from alibot_helpers.github_utilities import setGithubStatus, parseGithubRef
from alibot_helpers.github_utilities import GithubCachedClient, PickledCache
from alibot_helpers.utilities import to_unicode

# Allow uploading logs to S3
try:
    from boto3.s3.transfer import S3Transfer
    import boto3
except ImportError:
    pass


def parse_args():
    parser = ArgumentParser()
    parser.add_argument("--work-dir", "-w", default="sw", dest="workDir")

    parser.add_argument("--default", default="release")

    parser.add_argument("--devel-prefix", "-z",
                        dest="develPrefix",
                        default="")

    parser.add_argument("--pr",
                        required=True,
                        help=("Pull request which was checked in "
                              "<org>/<project>#<nr>@ref format"))

    parser.add_argument("--no-comments",
                        action="store_true",
                        dest="noComments",
                        default=False,
                        help="Use Details button, do not post a comment")

    status_gp = parser.add_mutually_exclusive_group()

    status_gp.add_argument("--success",
                           action="store_true",
                           dest="success",
                           default=False,
                           help="Signal a green status, not error")

    status_gp.add_argument("--pending",
                           action="store_true",
                           help="Signal only that the build has started")

    parser.add_argument("--status", "-s",
                        required=True,
                        help="Check which had the error")

    parser.add_argument("--dry-run", "-n",
                        action="store_true",
                        default=False,
                        help="Do not actually comment")

    parser.add_argument("--limit", "-l",
                        default=50,
                        help="Max number of lines from the report")

    parser.add_argument("--message", "-m",
                        dest="message",
                        help="Message to be posted")

    parser.add_argument("--logs-dest",
                        dest="logsDest",
                        default="rsync://repo.marathon.mesos/store/logs",
                        help="Destination store for logs. Either rsync://<rsync server enpoint> or s3://<bucket>.<server>")

    parser.add_argument("--log-url",
                        dest="logsUrl",
                        default="https://ali-ci.cern.ch/repo/logs",
                        help="Destination path for logs")

    parser.add_argument("--github-cache-file", default=expanduser("~/.cached-commits"),
                        help="Where to cache GitHub API responses (default %(default)s)")

    parser.add_argument("--debug", "-d",
                        action="store_true",
                        default=False,
                        help="Turn on debug output")

    args = parser.parse_args()
    if "#" not in args.pr:
        parser.error("You need to specify a pull request")
    if "@" not in args.pr:
        parser.error("You need to specify a commit this error refers to")
    return args


class Logs(object):
    def __init__(self, args, is_branch):
        self.work_dir = args.workDir
        self.develPrefix = args.develPrefix
        self.limit = args.limit
        self.norm_status = re.sub('[^a-zA-Z0-9_-]', '_', args.status)
        self.full_log = self.constructFullLogName(args.pr)
        self.is_branch = is_branch
        self.full_log_latest = self.constructFullLogName(args.pr, latest=True)
        self.pretty_log = self.constructFullLogName(args.pr, pretty=True)
        self.dest = args.logsDest
        self.url = join(args.logsUrl, self.pretty_log)
        self.log_url = join(args.logsUrl, self.full_log)
        self.build_successful = args.success
        self.pr_built = parse_pr(args.pr)

    def parse(self):
        self.find()
        self.grep()
        self.cat(self.full_log)
        self.generate_pretty_log()
        if self.is_branch:
            self.cat(self.full_log_latest, no_delete=True)
        if self.dest.startswith("rsync:"):
            self.rsync(self.dest)
        elif self.dest.startswith("s3"):
            self.s3Upload(self.dest)
        else:
            print("Unknown destination url %s" % self.dest)

    def constructFullLogName(self, pr, latest=False, pretty=False):
        # file to which we cat all the individual logs
        pr = parse_pr(pr)
        return join(pr.repo_name, pr.id, "latest" if latest else pr.commit, self.norm_status,
                    "pretty.html" if pretty else "fullLog.txt")

    def find(self):
        search_path = join(self.work_dir, "BUILD/*latest*/log")
        print("Searching all logs matching: %s" % search_path, file=sys.stderr)
        suffix = ("latest-" + self.develPrefix).rstrip("-")
        self.logs = {x for x in glob(search_path)
                     if dirname(x).endswith(suffix)}
        print("Found:\n%s" % "\n".join(self.logs), file=sys.stderr)

    def grepall(self, regex, context_before=3, context_after=3,
                ignore_log_files=()):
        '''Grep logfiles for regex, keeping context lines around matches.

        Each logfile is searched for lines matching regex. If a line matches,
        it is returned, together with the preceding context_before and
        following context_after lines. Overlapping context lines are only
        returned once. File names matching a glob pattern in ignore_log_files
        are not searched.

        Matching lines and context lines from all files are returned
        concatenated into a single string.
        '''
        context_sep = '--\n'
        out_lines = []
        for log in self.logs:
            if any(fnmatch(log, ignore) for ignore in ignore_log_files):
                continue
            # This deque will discard old lines when new ones are appended.
            context_lines = deque(maxlen=context_before)
            future_context = 0
            first_match = True
            try:
                with open(log) as logf:
                    for line in logf:
                        if re.search(regex, line):
                            # If this is the first match in this file, print
                            # the file name header. This means we get no header
                            # if nothing in this file matches, as intended.
                            if first_match:
                                out_lines.append('## %s\n' % log)
                                first_match = False
                            if future_context <= 0:
                                # If we're not currently in context, start a
                                # new block and output the last few lines.
                                out_lines.append(context_sep)
                                out_lines.extend(context_lines)
                                # The current line is output below.
                            future_context = context_after + 1
                        context_lines.append(line)
                        if future_context > 0:
                            out_lines.append(line)
                            future_context -= 1
                # If we matched at least once, we must've output at least one
                # block, so close it here by outputting a block separator line,
                # and separate files from each other using newlines.
                if not first_match:
                    out_lines.append(context_sep + '\n\n')
            except Exception as err:
                out_lines.append('\n!!! {} parsing {}: {}\n\n'
                                 .format(type(err), log, err))
        return ''.join(out_lines)

    def grep(self):
        """Grep for errors in the build logs, or, if none are found,
        return the last N lines where N is the limit argument.

        Also extract errors from failed unit tests and o2checkcode, and various
        other helpful messages.
        """
        error_log_lines = []
        for log in self.logs:
            err, out = getstatusoutput("grep -he ': error:' -A 3 -B 3 -- %s "
                                       "|| tail -n %s %s" % (log, self.limit, log))
            if err:
                print("Error while parsing logs", out, sep="\n", file=sys.stderr)
                continue
            error_log_lines.append(log)
            error_log_lines.extend(out.split("\n"))
        self.error_log = "\n".join(error_log_lines[:self.limit]).strip(" \n\t")

        # Messages from the general error/warning logs are reported in
        # o2checkcode_messages as well, so don't report them twice. The general
        # logs also contain false positives, so o2checkcode_messages is better.
        self.errors_log = self.grepall(
            r': (internal compiler |fatal )?error:|^Error(:| in )|'
            r'make.*: \*\*\*|\*\*\*Failed',
            ignore_log_files=['*/o2checkcode-latest*/log'])
        self.warnings_log = self.grepall(
            r': warning:|^Warning: (?!Unused direct dependencies:$)',
            ignore_log_files=['*/o2checkcode-latest*/log'])
        self.o2checkcode_messages = self.grepall(
            r'=== List of errors found ===',
            context_before=0, context_after=float('inf'))
        self.failed_unit_tests = self.grepall(
            r'Test *#[0-9]*: .*\*\*\*Failed|%% tests passed')
        self.compiler_killed = self.grepall(
            r'fatal error: Killed signal terminated program')
        self.cmake_errors = self.grepall(
            r'^CMake Error', context_before=0, context_after=10)
        # These two sections are for the O2 full system test.
        self.fst_task_timeout = self.grepall(
            r'task timeout reached', context_before=3, context_after=0)
        self.full_system_test = self.grepall(
            r'Detected critical problem in logfile',
            context_before=0, context_after=20)

    def generate_pretty_log(self):
        '''Extract error messages from logs.

        The errors are presented on a user-friendly HTML page, to be uploaded
        alongside the full message log.
        '''
        def htmlescape(string):
            return string.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')

        def display(string):
            return 'block' if string else 'none'

        with open(join('copy-logs', self.pretty_log), 'w') as f:
            f.write(PRETTY_LOG_TEMPLATE % {
                'status': 'succeeded' if self.build_successful else 'failed',
                'log_url': self.log_url,
                'o2checkcode': htmlescape(self.o2checkcode_messages),
                'o2c_display': display(self.o2checkcode_messages),
                'unittests': htmlescape(self.failed_unit_tests),
                'unit_display': display(self.failed_unit_tests),
                'errors': htmlescape(self.errors_log),
                'err_display': display(self.errors_log),
                'warnings': htmlescape(self.warnings_log),
                'warn_display': display(self.warnings_log),
                'cmake': htmlescape(self.cmake_errors),
                'cmake_display': display(self.cmake_errors),
                'killed_display': display(self.compiler_killed),
                'noerrors_display': display(not any((
                    self.o2checkcode_messages, self.failed_unit_tests,
                    self.errors_log, self.warnings_log, self.compiler_killed,
                    self.cmake_errors))),
                'fst_logfile': htmlescape(self.full_system_test),
                'fst_timeout': htmlescape(self.fst_task_timeout),
                'fst_display': display(self.full_system_test or self.fst_task_timeout),
                'hostname': htmlescape(platform.node()),
                'finish_time': datetime.datetime.utcnow().strftime('%a %-d %b %Y, %H:%M:%S UTC'),
                'repo': self.pr_built.repo_name,
                'pr_id': self.pr_built.id,
                'commit': self.pr_built.commit,
                'login_cmd': re.sub(
                    '^(?:thermos-)?([a-z]*)-([a-z]*)-([a-z0-9_-]*)-([0-9]*)(?:-[0-9a-f]*){5}$',
                    r'aurora task ssh -l root build/\1/\2/\3/\4',
                    os.getenv('MESOS_EXECUTOR_ID', '(no Mesos ID)')),
            })

    def cat(self, target_file, no_delete=False):
        if not no_delete:
            def print_delete_error(func, path, exc):
                print(func.__name__, "could not delete", path,
                      sep=": ", file=sys.stderr)
            shutil.rmtree("copy-logs", onerror=print_delete_error)
        try:
            os.makedirs(dirname(join("copy-logs", target_file)))
        except OSError as err:
            # Directory already exists. OSError is raised on python2 and
            # FileExistsError on python3; the latter is a subclass of OSError,
            # so this clause handles both.
            print("cannot create target dir:", err, file=sys.stderr)

        with open(join("copy-logs", target_file), "w") as logf:
            print("Finished building on", platform.node(), "at",
                  datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S"),
                  file=logf)
            print("Built commit", self.pr_built.commit, "from",
                  "https://github.com/%s/pull/%s" % (self.pr_built.repo_name,
                                                     self.pr_built.id),
                  file=logf)
            if "MESOS_EXECUTOR_ID" in os.environ:
                mesos_id = os.environ["MESOS_EXECUTOR_ID"]
                print("To log into the build machine, use:\n",
                      "   aurora task ssh -l root",
                      re.sub("^(?:thermos-)?([a-z]*)-([a-z]*)-([a-z0-9_-]*)-([0-9]*)(?:-[0-9a-f]*){5}$",
                             r"build/\1/\2/\3/\4", mesos_id),
                      file=logf)
            if self.logs:
                print("The following files are present in the log:", file=logf)
                for log in self.logs:
                    print("-", log, file=logf)
            else:
                print("No logs found. Please check the aurora log.",
                      "See http://alisw.github.io/infrastructure-pr-testing for more instructions.",
                      sep="\n", file=logf)
            for log in self.logs:
                print("## Begin", log, file=logf)
                with open(log) as sublogf:
                    logf.writelines(sublogf)
                print("## End", log, file=logf)

    def rsync(self, dest):
        err, out = getstatusoutput("cd copy-logs && rsync -av ./ %s" % dest)
        if err:
            print("Error while copying logs to store.", file=sys.stderr)
            print(out, file=sys.stderr)

    def s3Upload(self, dest):
        m = re.compile("^s3://([^.]+)[.](.*)").match(dest)
        bucket_name = m.group(1)
        server = m.group(2)
        s3_client = boto3.client('s3',
                                 aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
                                 aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
                                 endpoint_url="https://%s" % (server))
        s3_client.list_buckets()
        transfer = S3Transfer(s3_client)
        matches = []
        for root, dirnames, filenames in os.walk('copy-logs'):
            for filename in filenames:
                matches.append(join(root, filename))
        for src in matches:
            try:
                dst = re.compile('^copy-logs/').sub('', src)
                transfer.upload_file(src, bucket_name, dst, extra_args={
                    'ContentType': 'text/html' if src.endswith('.html') else 'text/plain',
                    'ContentDisposition': 'inline'})
            except Exception as e:
                print("Failed to upload %s to %s in bucket %s.%s" % (src, dst, bucket_name, server))


def get_pending_namespace(args):
    '''Return a Namespace to set status to pending with an appropriate message.'''
    return Namespace(commit=args.pr,
                     status=args.status + "/pending",
                     message=args.message,
                     url="")


def handle_branch(cgh, pr, logs, args):
    ns = get_pending_namespace(args) if args.pending else \
        Namespace(commit=args.pr,
                  status=args.status + "/error",
                  message="",
                  url="")
    setGithubStatus(cgh, ns)
    sys.exit(0)


def handle_pr_id(cgh, pr, logs, args):
    commit = cgh.get("/repos/{repo_name}/commits/{ref}",
                     repo_name=pr.repo_name,
                     ref=pr.commit)
    sha = commit["sha"]

    if not args.pending:
        message = "Error while checking %s for %s at %s:\n" % (args.status, sha, datetime.datetime.now().strftime("%Y-%m-%d %H:%M"))
        if args.message:
            message += args.message
        else:
            message += "```\n%s\n```\nFull log [here](%s).\n" % (to_unicode(logs.error_log), to_unicode(logs.url))

    if args.dry_run:
        # commit does not exist...
        print("Will annotate %s" % commit["sha"])
        print(message)
        sys.exit(0)

    # Set status
    ns = get_pending_namespace(args) if args.pending else \
        Namespace(commit=args.pr,
                  status=args.status + ("/success" if args.success else "/error"),
                  message="",
                  url=logs.url)
    setGithubStatus(cgh, ns)

    # Comment if appropriate
    if args.noComments or args.success or args.pending:
        return

    prIssueComments = cgh.get("/repos/{repo_name}/issues/{pr_id}/comments",
                              repo_name=pr.repo_name,
                              pr_id=pr.id)

    messageHash = calculateMessageHash(message)
    for comment in prIssueComments:
        if comment["body"].startswith("Error while checking %s for %s" % (args.status, sha)):
            if calculateMessageHash(comment["body"]) != messageHash:
                print("Comment was different. Updating", file=sys.stderr)
                cgh.patch(
                    "/repos/{repo_name}/issues/comments/{commentID}",
                    {"body": message},
                    repo_name=pr.repo_name,
                    commentID=comment["id"]
                )
                sys.exit(0)

            print("Found same comment for the same commit", file=sys.stderr)
            sys.exit(0)


    cgh.post(
        "repos/{repo_name}/issues/{pr_id}/comments",
        {"body": message},
        repo_name=pr.repo_name,
        pr_id=pr.id
    )


def parse_pr(pr):
    repo_name, pr_id, pr_commit = parseGithubRef(pr)
    return Namespace(repo_name=repo_name,
                     id=pr_id,
                     commit=pr_commit)


def main():
    args = parse_args()
    pr = parse_pr(args.pr)
    logs = Logs(args, is_branch=not pr.id.isdigit())
    if not args.message and not args.pending:
        logs.parse()

    cache = PickledCache(args.github_cache_file)
    with GithubCachedClient(token=github_token(), cache=cache) as cgh:
        # If the branch is not a PR, we should look for open issues
        # for the branch. This should really folded as a special case
        # of the PR case.
        func = handle_branch if not pr.id.isdigit() else handle_pr_id
        func(cgh, pr, logs, args)

    cgh.printStats()


PRETTY_LOG_TEMPLATE = '''\
<!DOCTYPE html>
<head>
  <title>Build results</title>
  <style>
   body { border-width: 0.5rem; border-style: solid; margin: 0; padding: 1rem; min-height: 100vh; box-sizing: border-box; }
   pre { overflow-x: auto; }
   .succeeded { border-color: #1b5e20; }
   .succeeded h1 { color: #1b5e20; }
   .failed { border-color: #b71c1c; }
   .failed h1 { color: #b71c1c; }
   #noerrors, #noerrors-toc { display: %(noerrors_display)s; }
   #compiler-killed, #compiler-killed-toc { display: %(killed_display)s; }
   #o2checkcode, #o2checkcode-toc { display: %(o2c_display)s; }
   #tests, #tests-toc { display: %(unit_display)s; }
   #errors, #errors-toc { display: %(err_display)s; }
   #warnings, #warnings-toc { display: %(warn_display)s; }
   #cmake, #cmake-toc { display: %(cmake_display)s; }
   #fullsystest, #fullsystest-toc { display: %(fst_display)s; }
  </style>
</head>
<body class="%(status)s">
  <h1>The build %(status)s</h1>
  <table>
    <tr><th>Build host</th><td>%(hostname)s</td></tr>
    <tr><th>Finished at</th><td>%(finish_time)s</td></tr>
    <tr><th>Built PR</th>
        <td><a href="https://github.com/%(repo)s/pull/%(pr_id)s">%(repo)s#%(pr_id)s</a></td></tr>
    <tr><th>Built commit</th>
        <td><a href="https://github.com/%(repo)s/pull/%(pr_id)s/commits/%(commit)s"><code>%(commit)s</code></a></td></tr>
  </table>
  <p>Use the following command to log in to the build machine:
     <pre><code>%(login_cmd)s</code></pre></p>
  <p>The code that finds and extracts error messages may have missed some.</p>
  <p>Check the <a href="%(log_url)s">full build log</a> if you suspect the build failed for reasons not listed below.</p>
  <h3>Table of contents</h3>
  <p><nav><ol>
    <li id="noerrors-toc"><a href="#noerrors">No errors found</a></li>
    <li id="compiler-killed-toc"><a href="#compiler-killed">Error: the compiler process was killed</a></li>
    <li id="cmake-toc"><a href="#cmake">CMake errors and warnings</a></li>
    <li id="o2checkcode-toc"><a href="#o2checkcode"><code>o2checkcode</code> results</a></li>
    <li id="tests-toc"><a href="#tests">Unit test results</a></li>
    <li id="fullsystest-toc"><a href="#fullsystest">O2 full system test</a></li>
    <li id="errors-toc"><a href="#errors">Error messages</a></li>
    <li id="warnings-toc"><a href="#warnings">Compiler warnings</a></li>
  </ol></nav></p>
  <section id="noerrors">
    <h2>No errors found</h2>
    <p>Check the <a href="%(log_url)s">full build log</a> to see all compilation messages.</p>
  </section>
  <section id="compiler-killed">
    <h2>Error: the compiler process was killed</h2>
    <p>This can happen in case of memory pressure. A later automatic build may solve this.</p>
  </section>
  <section id="o2checkcode">
    <h2><code>o2checkcode</code> results</h2>
    <p><pre><code>%(o2checkcode)s</code></pre></p>
  </section>
  <section id="tests">
    <h2>Unit test results</h2>
    <p><pre><code>%(unittests)s</code></pre></p>
  </section>
  <section id="fullsystest">
    <h2>O2 full system test</h2>
    <p><pre><code>%(fst_timeout)s</code></pre></p>
    <p><pre><code>%(fst_logfile)s</code></pre></p>
  </section>
  <section id="cmake">
    <h2>CMake errors</h2>
    <p><pre><code>%(cmake)s</code></pre></p>
  </section>
  <section id="errors">
    <h2>Error messages</h2>
    <p>Note that the following list may include false positives! Check the sections above first.</p>
    <p><pre><code>%(errors)s</code></pre></p>
  </section>
  <section id="warnings">
    <h2>Compiler warnings</h2>
    <p>Note that the following list may include false positives! Check the sections above first.</p>
    <p><pre><code>%(warnings)s</code></pre></p>
  </section>
  <p>Check the <a href="%(log_url)s">full build log</a> for potentially more helpful messages.</p>
</body>
'''

if __name__ == "__main__":
    main()
