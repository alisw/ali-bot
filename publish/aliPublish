#!/usr/bin/env python

import logging, sys, json, yaml, requests, urllib3, errno
from glob import glob
from argparse import ArgumentParser
from commands import getstatusoutput
from datetime import datetime
from requests import RequestException
from time import sleep, time
from yaml import YAMLError
from logging import debug, error, info
from re import search, escape, sub
from os.path import isdir, isfile, realpath, dirname, getmtime, join, basename
from os import chmod, remove, chdir, getcwd, getpid, kill, makedirs, rename
from shutil import rmtree
from tempfile import NamedTemporaryFile, mkdtemp
from subprocess import Popen, PIPE, STDOUT
from smtplib import SMTP
from socket import getfqdn
from random import random, choice, shuffle
from urlparse import urlsplit, urlunsplit

def format(s, **kwds):
  return s % kwds

def rmrf(path):
  try:
    if isdir(path):
      rmtree(path)
    elif isfile(path):
      remove(path)
  except OSError as e:
    debug(format("When deleting %(path)s: %(msg)s (ignored)",
                 path=path, msg=str(e)))

def searchMany(name, exprs):
  if isinstance(exprs, list):
    for e in exprs:
      if search(e, name): return True
  elif exprs == True:
    return True
  return False

def applyFilter(name, timestamp, includeRules, excludeRules, includeFirst, excludeOlderThanSec):
  if excludeOlderThanSec > 0 and time()-timestamp > excludeOlderThanSec:
    return False  # excluding because too old (excludeOlderThanSec == 0: never exclude)
  if includeFirst:
    if searchMany(name, includeRules):
      return not searchMany(name, excludeRules)
    else:
      return False
  else:
    if searchMany(name, excludeRules):
      return False
    else:
      if includeRules is None:
        # process exclude first, and no explicit include rule: keep it
        return True
      else:
        return searchMany(name, includeRules)

def runInstallScript(script, dryRun, **kwsub):
  if dryRun:
    debug(format("Dry run: publish script follows:\n" + script, **kwsub))
    return 0
  with NamedTemporaryFile(delete=False) as fp:
    fn = fp.name
    fp.write(format(script, **kwsub))
  chmod(fn, 0o700)
  debug(format("Created unpack script: %(file)s", file=fn))
  rv = execute(fn)
  remove(fn)
  debug(format("Unpack script %(file)s returned %(rv)d", file=fn, rv=rv))
  return rv

def execute(command):
  popen = Popen(command, shell=False, stdout=PIPE, stderr=STDOUT)
  linesIterator = iter(popen.stdout.readline, "")
  for line in linesIterator:
    debug(line.strip("\n"))  # yield line
  output = popen.communicate()[0]
  debug(output)
  exitCode = popen.returncode
  return exitCode

def grabOutput(command):
  debug("Executing command: " + " ".join(command))
  popen = Popen(command, shell=False, stdout=PIPE, stderr=STDOUT)
  out = popen.communicate()[0]
  return (popen.returncode, out)

class JGet(object):
  def __init__(self, http_ssl_verify, conn_timeout_s, conn_retries, conn_dethrottle_s, cache_dir):
    self.http_ssl_verify   = http_ssl_verify
    self.conn_timeout_s    = conn_timeout_s
    self.conn_retries      = conn_retries
    self.conn_dethrottle_s = conn_dethrottle_s
    self.last_ts           = time()
    self.count_cached      = 0
    self.count_req         = 0
    self.count_req_retries = 0
    self.cache_dir         = cache_dir
    if cache_dir:
      try:
        makedirs(cache_dir)
      except OSError as exc:
        if not isdir(cache_dir) or exc.errno != errno.EEXIST:
          error(format("Cannot create cache dir for package deps %(cache)s: disabling",
                       cache=cache_dir))
          self.cache_dir = None
    self.urls              = []
    self.cachable          = '/[^/]+/(dist|dist-runtime|dist-direct)/[^/]+/[^/]+/$'
  def __call__(self, url):
    self.count_req += 1
    dethrottle = self.conn_dethrottle_s
    cache_file = None
    cache_status = "DIR"
    m = search(self.cachable, url)
    if self.cache_dir and m:
      cache_file = join(self.cache_dir, sub("[^A-Za-z0-9-_]", "_", m.group(0)).strip("_")+".json")
      try:
        j = json.loads(open(cache_file).read())
        debug("Using cached data for %s" % url)
        self.count_cached += 1
        self.urls.append({"url":url, "cached":"HIT"})
        return j
      except (IOError,ValueError):
        pass

    self.urls.append({"url":url, "cached": "MIS" if m else "DIR"})
    for i in range(0,self.conn_retries+1):
      pause_s = max(dethrottle-(time()-self.last_ts), 0)
      debug(format("Dethrottling connection to %(url)s: pausing %(pause).2f second(s)",
                   url=url, pause=pause_s))
      sleep(pause_s)
      try:
        self.count_req_retries += 1
        j = requests.get(url,
                         verify=self.http_ssl_verify,
                         timeout=self.conn_timeout_s).json()
      except ValueError:
        j = {}
      except RequestException as e:
        error(format("Error getting %(url)s, %(att)d attempt(s) left: %(msg)s",
                     url=url,
                     att=self.conn_retries-i,
                     msg=str(e)))
        dethrottle = 2*dethrottle
        j = None
      self.last_ts = time()
      if j is not None:
        if cache_file:
          try:
            with open(cache_file, "w") as jc:
              jc.write(json.dumps(j))
          except IOError as e:
            error("Cannot cache %s: %s" % (url,e))
        return j
    return {}

class PublishException(Exception):
  pass

class PlainFilesystem(object):

  def __init__(self, modulefileTpl, pkgdirTpl, publishScriptTpl,
                     connParams, dryRun=False):

    self._repository         = ""
    self._modulefileTpl      = modulefileTpl
    self._pkgdirTpl          = pkgdirTpl
    self._publishScriptTpl   = publishScriptTpl
    self._connParams         = connParams
    self._dryRun             = dryRun
    self._countChanges       = 0

  def _kw(self, url, arch, pkgName, pkgVer):
    kw =  { "url": url, "package": pkgName, "version": pkgVer, "repo": self._repository or "filesystem",
            "arch": arch }
    kw.update({ "pkgdir": format(self._pkgdirTpl, **kw) })
    kw.update({ "modulefile": format(self._modulefileTpl, **kw) })
    kw.update(self._connParams)
    kw["http_ssl_verify"] = 1 if kw["http_ssl_verify"] else 0
    return kw

  def installed(self, arch, pkgName, pkgVer):
    kw = self._kw(None, arch, pkgName, pkgVer)
    debug(format("%(repo)s: checking if %(package)s %(version)s is installed for %(arch)s", **kw))
    return isdir(kw["pkgdir"]) or isfile(kw["modulefile"])

  def install(self, url, arch, pkgName, pkgVer, deps, allDeps):
    kw = self._kw(url, arch, pkgName, pkgVer)
    rv = runInstallScript(self._publishScriptTpl, self._dryRun, **kw)
    if rv == 0:
      self._countChanges += 1
    else:
      self._cleanup(arch, pkgName, pkgVer)
    return rv

  def _cleanup(self, arch, pkgName, pkgVer):
    kw = self._kw(None, arch, pkgName, pkgVer)
    debug(format("%(repo)s: cleaning up %(pkgdir)s and %(modulefile)s", **kw))
    rmrf(kw["pkgdir"])
    rmrf(kw["modulefile"])

  def transaction(self):
    return True

  def abort(self):
    return True

  def publish(self):
    return True


class CvmfsServer(PlainFilesystem):

  def __init__(self, repository, modulefileTpl, pkgdirTpl, publishScriptTpl,
                     connParams, dryRun=False):
    super(CvmfsServer, self).__init__(modulefileTpl, pkgdirTpl,
                                      publishScriptTpl, connParams, dryRun)
    self._inCvmfsTransaction = False
    self._repository         = repository

  def transaction(self):
    if self._inCvmfsTransaction:
      debug(format("%(repo)s: already in a transaction", repo=self._repository))
      return True
    elif self._dryRun:
      info(format("%(repo)s: started transaction (dry run)", repo=self._repository))
      self._inCvmfsTransaction = True
      return True
    else:
      if execute([ "cvmfs_server", "transaction", self._repository ]) == 0:
        info(format("%(repo)s: started transaction", repo=self._repository))
        self._inCvmfsTransaction = True
        return True
      error(format("%(repo)s: cannot commence transaction: maybe another one is in progress?",
                   repo=self._repository))
      return False

  def abort(self, force=False):
    if not self._inCvmfsTransaction and not force:
      debug(format("%(repo)s: no transaction to abort", repo=self._repository))
      return True
    if self._dryRun and not force:
      info(format("%(repo)s: transaction aborted (dry run)", repo=self._repository))
      self._inCvmfsTransaction = False
      return True
    rv = execute([ "cvmfs_server", "abort", "-f", self._repository ])
    if rv == 0:
      info(format("%(repo)s: transaction aborted", repo=self._repository))
      self._inCvmfsTransaction = False
      return True
    error(format("%(repo)s: cannot abort transaction", repo=self._repository))
    return False

  def publish(self):
    if not self._inCvmfsTransaction:
      debug(format("%(repo)s: not in a transaction", repo=self._repository))
      return True
    if not self._countChanges:
      debug(format("%(repo)s: nothing to publish, cancelling transaction", repo=self._repository))
      return self.abort()
    info(format("%(repo)s: publishing transaction, %(npkg)d new package(s)",
                repo=self._repository, npkg=self._countChanges))
    if self._dryRun:
      info(format("%(repo)s: transaction published (dry run)", repo=self._repository))
      return True
    rv = execute([ "cvmfs_server", "publish", self._repository ])
    if rv == 0:
      info(format("%(repo)s: transaction published!", repo=self._repository))
      self._inCvmfsTransaction = False
      return True
    else:
      error(format("%(repo)s: cannot publish CVMFS transaction, aborting",
            repo=self._repository))
      self.abort()
      return False


class AliEnPackMan(object):

  def __init__(self, publishScriptTpl, connParams, dryRun=False):
    self._dryRun = dryRun
    self._publishScriptTpl = publishScriptTpl
    self._packs = None
    self._connParams = connParams
    self._cachedArchs = []

  def _kw(self, url, arch, pkgName, pkgVer, deps):
    kw =  { "url": url, "package": pkgName, "version": pkgVer, "arch": arch, "dependencies": deps }
    kw.update(self._connParams)
    kw["http_ssl_verify"] = 1 if kw["http_ssl_verify"] else 0
    return kw

  def installed(self, arch, pkgName, pkgVer):
    kw = self._kw(None, arch, pkgName, pkgVer, None)
    debug(format("PackMan: checking if %(package)s %(version)s is installed for %(arch)s", **kw))

    if self._packs is None:
      self._packs = {}
      for line in grabOutput([ "alien", "-exec",
                               "packman", "list", "-all", "-force" ])[1].split("\n"):
        m = search(r"VO_ALICE@(.+?)::([^\s]+)", line)
        if not m: continue
        pkg = m.group(1)
        ver = m.group(2)
        if not pkg in self._packs:
          self._packs[pkg] = {}
        self._packs[pkg].update({ver: []})

    if not self._packs:
      raise PublishException("PackMan: could not get list of packages from AliEn this time")

    if not arch in self._cachedArchs:
      for line in grabOutput([ "alien", "-exec", "find", "/alice/packages", arch ])[1].split("\n"):
        m = search(r"^/alice/packages/([^/]+)/([^/]+)/", line)
        if not m: continue
        pkg = m.group(1)
        ver = m.group(2)
        if not pkg in self._packs: continue
        self._packs[pkg].get(ver, []).append(arch)
      self._cachedArchs.append(arch)

    return arch in self._packs.get(pkgName, {}).get(pkgVer, [])

  def install(self, url, arch, pkgName, pkgVer, deps, allDeps):
    kw = self._kw(url, arch, pkgName, pkgVer,
                  ",".join(["VO_ALICE@"+x["name"]+"::"+x["ver"] for x in deps]))
    return runInstallScript(self._publishScriptTpl, self._dryRun, **kw)

  def transaction(self):
    # Not actually opening a "transaction", but failing if AliEn appears down.
    # If we don't fail here, package list appears empty and we'll attempt to
    # publish *every* package, and failing...
    _,out = grabOutput([ "alien", "-exec", "ls", "/alice/packages" ])
    if "AliRoot" in out.split("\n"):
      debug("PackMan: AliEn connection and APIs appear to work")
      return True
    error("PackMan: API response incorrect, assuming AliEn is not working at the moment")
    return False

  def abort(self, force=False):
    return True

  def publish(self):
    return True

class RPM(object):

  def __init__(self, repoDir, publishScriptTpl, connParams, genUpdatableRpms, dryRun=False):
    self._dryRun = dryRun
    self._genUpdatableRpms = genUpdatableRpms
    self._repoDir = repoDir
    self._stageDir = join(repoDir, "staging")
    self._publishScriptTpl = publishScriptTpl
    self._countChanges = 0
    self._connParams = connParams
    self._archs = []
    self._inRpmTransaction = False

  def _kw(self, url, arch, pkgName, pkgVer, workDir=None, stageDir=None, deps=None):
    kw =  { "url"          : url,
            "package"      : pkgName,
            "version"      : pkgVer,
            "version_rpm"  : pkgVer.replace("-", "_"),
            "arch"         : arch,
            "dependencies" : deps,
            "repodir"      : self._repoDir+"/"+arch,
            "workdir"      : workDir,
            "stagedir"     : stageDir,
            "updatable"    : "1" if self._genUpdatableRpms else "" }
    kw.update(self._connParams)
    kw["http_ssl_verify"] = 1 if kw["http_ssl_verify"] else 0
    if self._genUpdatableRpms:
      kw.update({ "rpm": format("alisw-%(package)s-%(version_rpm)s-1.%(arch)s.rpm", **kw) })
    else:
      kw.update({ "rpm": format("alisw-%(package)s+%(version)s-1-1.%(arch)s.rpm", **kw) })
    return kw

  def installed(self, arch, pkgName, pkgVer):
    kw = self._kw(None, arch, pkgName, pkgVer)
    debug(format("RPM: checking if %(rpm)s exists for %(package)s %(version)s on %(arch)s", **kw))
    return isfile(format("%(repodir)s/%(rpm)s", **kw))

  def install(self, url, arch, pkgName, pkgVer, deps, allDeps):
    # All RPMs are created in workDir (temporary, destroyed after creation). Eventually they are
    # moved to stageDirArch (not the repository)

    stageDirArch = join(self._stageDir, arch)
    if not self._dryRun:
      workDir = mkdtemp(prefix=join(self._stageDir, "tmp", "aliPublish-RPM-"))
    else:
      workDir = "/dryrun_doesnotexist"

    kw = self._kw(url, arch, pkgName, pkgVer, workDir, stageDirArch,
                  " ".join(["alisw-%s+%s" % (x["name"], x["ver"]) for x in deps]))

    if not self._dryRun:
      debug(format("RPM: created temporary working directory %(workdir)s", **kw))
      debug(format("RPM: creating staging directory %(staging)s", staging=stageDirArch))
      try:
        makedirs(stageDirArch)
      except OSError as exc:
        if not isdir(stageDirArch) or exc.errno != errno.EEXIST:
          error(format("RPM: error creating staging directory %(staging)s", staging=stageDirArch))
          return 1

    rv = runInstallScript(self._publishScriptTpl, self._dryRun, **kw)
    if rv == 0:
      if not arch in self._archs:
        self._archs.append(arch)
      self._countChanges += 1
    debug(format("RPM: removing temporary working directory %(workdir)s", **kw))
    rmrf(workDir)
    return rv

  def transaction(self):
    if not self._inRpmTransaction and not self._dryRun:
      self._inRpmTransaction = True
      debug(format("RPM: cleaning up staging dir %(staging)s", staging=self._stageDir))
      rmrf(self._stageDir)
      stagingTmp = join(self._stageDir, "tmp")
      debug(format("RPM: creating staging tempdir %(staging)s", staging=stagingTmp))
      try:
        makedirs(stagingTmp)
      except OSError as exc:
        if not isdir(stagingTmp) or exc.errno != errno.EEXIST:
          error(format("RPM: error creating staging tempdir %(staging)s", staging=stagingTmp))
          return False
    return True

  def abort(self, force=False):
    if self._inRpmTransaction and not self._dryRun:
      debug(format("RPM: aborting: cleaning up staging dir %(staging)s", staging=self._stageDir))
      rmrf(self._stageDir)
      self._inRpmTransaction = False
    return True

  def publish(self):
    if self._countChanges > 0:
      info(format("RPM: updating repository: %(npkgs)s new package(s)",
           npkgs=self._countChanges))
      if not self._dryRun:
        for arch in self._archs:
          cachedir = join(self._repoDir, "createrepo_cachedir", arch)
          archdir = join(self._repoDir, arch)
          stagedir = join(self._stageDir, arch)

          try:
            makedirs(cachedir)
          except OSError as exc:
            if not isdir(cachedir) or exc.errno != errno.EEXIST:
              error(format("RPM: error creating createrepo cachedir %(cachedir)s", cachedir=cachedir))
              return False

          try:
            makedirs(archdir)
          except OSError as exc:
            if not isdir(archdir) or exc.errno != errno.EEXIST:
              error(format("RPM: error creating repository dir %(archdir)s", archdir=archdir))
              return False

          info(format("RPM: moving newly created packages from %(staging)s to %(repo)s",
                      staging=stagedir, repo=archdir))
          for srcRpm in glob(join(stagedir, "*.rpm")):
            try:
              info(format("RPM: moving %(staged)s to %(archdir)s", staged=srcRpm, archdir=archdir))
              rename(srcRpm, join(archdir, basename(srcRpm)))
            except OSError:
              error(format("RPM: cannot move %(staged)s to %(archdir)s",
                    staged=srcRpm, archdir=archdir))

          # We can already remove the staging directory before calling `createrepo`. Errors here are
          # not fatal
          debug(format("RPM: removing staging directory %(staging)s", staging=self._stageDir))
          rmrf(self._stageDir)

          if execute([ "time", "createrepo", "--cachedir", cachedir, "--update", archdir ]) == 0:
            info(format("RPM: repository updated for %(arch)s", arch=arch))
          else:
            error(format("RPM: error updating repository for %(arch)s", arch=arch))
            return False
        return True
      elif self._dryRun:
        info("RPM: not updating repository, dry run")
        return True
      else:
        error("RPM: error updating repository")
        return False
    debug("RPM: nothing new to publish")
    return True

def nameVerFromTar(tar, arch, validPacks):
  for pkgName in validPacks:
    vre = format("^(%(pack)s)-(.*?)(\.%(arch)s\.tar\.gz)?$", pack=escape(pkgName), arch=arch)
    vm = search(vre, tar)
    if vm:
      return { "name": vm.group(1), "ver": vm.group(2) }
  return None

if hasattr(datetime, "total_seconds"):
  # Python 2.7+
  def getTotalSeconds(dt):
    return dt.total_seconds()
else:
  # Python 2.6
  def getTotalSeconds(dt):
    return (dt.microseconds + (dt.seconds + dt.days * 24 * 3600) * 1000000) / 1000000

def getTimestamp(s):
  # Convert a given string in the format "Thu, 03 Aug 2017 01:12:54 GMT" to
  # an UTC Unix timestamp and return it. Only strings ending with GMT or UTC
  # are supported! In case of errors, zero is returned
  if not s.endswith("GMT") and not s.endswith("UTC"):
    return 0
  try:
    dt = datetime.strptime(s, "%a, %d %b %Y %H:%M:%S %Z")
  except Exception as e:
    error("Cannot parse time string {timestring}: {exception}".format(
      timestring=s, exception=str(e)))
    return 0
  return getTotalSeconds(dt - datetime(1970, 1, 1))  # yes, it's a mess

def sync(pub, architectures, baseUrl, rules, excludeOlderThanSec, includeFirst, autoIncludeDeps,
         notifEmail, dryRun, jget):

  newPackages = {}

  # Template URLs
  packNamesUrlTpl   = "%(baseUrl)s/%(arch)s/dist-direct/"
  distUrlTpl        = "%(baseUrl)s/%(arch)s/dist/%(pack)s/%(pack)s-%(ver)s/"
  distDirectUrlTpl  = "%(baseUrl)s/%(arch)s/dist-direct/%(pack)s/%(pack)s-%(ver)s/"
  distRuntimeUrlTpl = "%(baseUrl)s/%(arch)s/dist-runtime/%(pack)s/%(pack)s-%(ver)s/"
  verUrlTpl         = "%(baseUrl)s/%(arch)s/dist-direct/%(pack)s/"
  getPackUrlTpl     = distDirectUrlTpl + "/%(pack)s-%(ver)s.%(arch)s.tar.gz"

  # Prepare the list of packages to install
  for arch in architectures:
    newPackages[arch] = []
    packNamesUrl = format(packNamesUrlTpl,
                          baseUrl=baseUrl, arch=arch)

    # Get valid package names for this architecture
    debug(format("Getting packages for architecture %(arch)s from %(url)s",
                 arch=arch, url=packNamesUrl))
    distPackages = [ p["name"] for p in jget(packNamesUrl) if p["type"] == "directory" ]
    distPackages.sort(key=lambda p: -len(p))
    debug("Packages found: %s" % ", ".join([p for p in distPackages]))

    # Packages to publish
    pubPackages = []

    # Get versions for all valid packages and filter them according to the rules
    for pkgName in distPackages:
      if includeFirst and pkgName not in rules["include"][arch]:
        continue
      if not includeFirst and rules["exclude"][arch].get(pkgName) == True:
        continue
      verUrl = format(verUrlTpl,
                      baseUrl=baseUrl, arch=arch, pack=pkgName)
      debug(format("%(arch)s / %(pack)s: listing versions under %(url)s",
                   arch=arch, pack=pkgName, url=verUrl))
      for pkgTar in jget(verUrl):
        if pkgTar["type"] != "directory":
          continue
        nameVer = nameVerFromTar(pkgTar["name"], arch, [pkgName])
        if nameVer is None:
          continue
        pkgVer = nameVer["ver"]
        pkgTime = getTimestamp(pkgTar["mtime"])  # UTC seconds since Unix epoch
        # Here we decide whether to include/exclude it
        if not applyFilter(pkgVer,
                           pkgTime,
                           rules["include"][arch].get(pkgName, None),
                           rules["exclude"][arch].get(pkgName, None),
                           includeFirst,
                           excludeOlderThanSec):
          debug(format("%(arch)s / %(pack)s / %(ver)s: excluded (timestamp: %(mtime)d)",
                arch=arch, pack=pkgName, ver=pkgVer, mtime=pkgTime))
          continue

        if not autoIncludeDeps:
          # Not automatically including dependencies, add this package only.
          # Not checking for dups because we can't have any
          pubPackages.append({ "name": pkgName, "ver": pkgVer })
          continue

        # At this point we have filtered in the package: let's see its dependencies!
        # Note that a package always depends on itself (list cannot be empty).
        distUrl = format(distRuntimeUrlTpl,
                         baseUrl=baseUrl, arch=arch, pack=pkgName, ver=pkgVer)
        runtimeDeps = jget(distUrl)
        if not runtimeDeps:
          error(format("%(arch)s / %(pack)s / %(ver)s: cannot list dependencies from %(url)s: skipping",
                       arch=arch, pack=pkgName, ver=pkgVer, url=distUrl))
          continue
        debug(format("%(arch)s / %(pack)s / %(ver)s: listing all dependencies under %(url)s",
                     arch=arch, pack=pkgName, ver=pkgVer, url=distUrl))
        for depTar in runtimeDeps:
          if depTar["type"] != "file":
            continue
          depNameVer = nameVerFromTar(depTar["name"], arch, distPackages)
          if depNameVer is None:
            continue
          depName = depNameVer["name"]
          depVer = depNameVer["ver"]
          # Append only if it does not exist yet
          if len([p for p in pubPackages if p["name"]==depName and p["ver"]==depVer]) == 0:
            debug(format("%(arch)s / %(pack)s / %(ver)s: adding %(depName)s %(depVer)s to publish",
                  arch=arch, pack=pkgName, ver=pkgVer, url=distUrl,
                  depName=depName, depVer=depVer))
            pubPackages.append({ "name": depName, "ver": depVer })

    pubPackages.sort(key=lambda itm: itm["name"])
    debug(format("%(arch)s: %(npacks)d package(s) candidate for publication: %(packs)s",
                 arch=arch, npacks=len(pubPackages),
                 packs=", ".join([p["name"]+" "+p["ver"] for p in pubPackages])))

    # Packages installation
    for pack in pubPackages:
      pkgUrl = format(getPackUrlTpl,
                       baseUrl=baseUrl, arch=arch, pack=pack["name"], ver=pack["ver"])

      if pub.installed(architectures[arch], pack["name"], pack["ver"]):
        debug(format("%(arch)s / %(pack)s / %(ver)s: already installed: skipping",
                     arch=arch, pack=pack["name"], ver=pack["ver"]))
        continue

      # Get direct and indirect dependencies
      deps = {}
      depUrlTpls = { "dist": distUrlTpl,
                     "dist-direct": distDirectUrlTpl,
                     "dist-runtime": distRuntimeUrlTpl }
      depFail = False
      for key,depsUrlTpl in depUrlTpls.iteritems():
        depsUrl = format(depsUrlTpl,
                         baseUrl=baseUrl, arch=arch, pack=pack["name"], ver=pack["ver"])
        debug(format("%(arch)s / %(pack)s / %(ver)s: listing %(key)s dependencies from %(url)s",
                     arch=arch, pack=pack["name"], ver=pack["ver"], key=key, url=depsUrl))
        jdeps = jget(depsUrl)
        if not jdeps:
          error(format("%(arch)s / %(pack)s / %(ver)s: cannot get %(dtype)s dependencies: skipping",
                       arch=arch, pack=pack["name"], ver=pack["ver"], dtype=key))
          newPackages[arch].append({ "name": pack["name"], "ver": pack["ver"], "success": False })
          depFail = True
          break
        deps[key] = [ nameVerFromTar(x["name"], arch, distPackages)
                      for x in jdeps if x["type"] == "file" ]
        deps[key] = [ x for x in deps[key] if (x is not None and
                                               x["name"] != pack["name"]) ]
      if depFail:
        continue
      # dist-direct-runtime: all entries in dist-direct but not in dist-runtime
      deps["dist-direct-runtime"] = [ x for x in deps["dist-direct"]
                                      if [ 1 for y in deps["dist-runtime"]
                                           if x["name"] == y["name"] ] ]

      # Here we can attempt the installation
      info(format("%(arch)s / %(pack)s / %(ver)s: getting and installing",
                  arch=arch, pack=pack["name"], ver=pack["ver"]))
      info(" * Source: %s" % pkgUrl)
      info(" * Direct deps: %s" % ", ".join([i["name"]+" "+i["ver"] for i in deps["dist-direct"]]))
      info(" * All deps: %s" % ", ".join([i["name"]+" "+i["ver"] for i in deps["dist"]]))
      info(" * Direct runtime deps: %s" % ", ".join([i["name"]+" "+i["ver"] for i in deps["dist-direct-runtime"]]))
      info(" * Runtime deps: %s" % ", ".join([i["name"]+" "+i["ver"] for i in deps["dist-runtime"]]))

      if not pub.transaction():
        sys.exit(2)  # fatal
      else:
        rv = pub.install(pkgUrl, architectures[arch], pack["name"], pack["ver"],
                         deps["dist-direct-runtime"], deps["dist-runtime"])
        newPackages[arch].append({ "name": pack["name"],
                                   "ver": pack["ver"],
                                   "success": (rv==0),
                                   "deps": deps["dist-direct-runtime"],
                                   "alldeps": deps["dist-runtime"] })
      if rv == 0:
        info(format("%(arch)s / %(pack)s / %(ver)s: installed successfully",
                     arch=arch, pack=pack["name"], ver=pack["ver"]))
      else:
        error(format("%(arch)s / %(pack)s / %(ver)s: publish script failed with %(rv)d",
                     arch=arch, pack=pack["name"], ver=pack["ver"], rv=rv))

  # Publish eventually
  if pub.publish():
    totSuccess = 0
    totFail = 0
    for arch,packStatus in newPackages.iteritems():
      nSuccess = sum([1 for x in packStatus if x["success"]])
      nFail = len(packStatus) - nSuccess
      totSuccess = totSuccess + nSuccess
      totFail = totFail + nFail
      info(format("%(arch)s: install OK for %(nSuccess)d/%(nPacks)d package(s): %(successPacks)s",
           arch=arch,
           nSuccess=nSuccess,
           nPacks=len(packStatus),
           successPacks=", ".join([x["name"]+" "+x["ver"] for x in packStatus if x["success"]])))
      if nFail:
        error(format("%(arch)s: install failed for %(nFail)d/%(nPacks)d package(s): %(failedPacks)s",
              arch=arch,
              nFail=nFail,
              nPacks=len(packStatus),
              failedPacks=", ".join([x["name"]+" "+x["ver"] for x in packStatus if not x["success"]])))
    if notifEmail:
      notify(notifEmail, architectures, newPackages, dryRun)
    else:
      debug("No email notification configured")
    return totFail == 0 or totSuccess > 0

  return False

def notify(conf, archs, pack, dryRun):
  if not "server" in conf:
    return
  try:
    mailer = SMTP(conf["server"], conf.get("port", 25))
  except Exception as e:
    error("Email notification: cannot connect to %s" % conf["server"])
    return
  for arch,packs in pack.iteritems():
    for p in packs:
      key = "success" if p["success"] else "failure"
      deps_fmt = "".join([ format(conf.get("package_format", "%(package)s %(version)s "),
                                   package=x["name"],
                                   version=x["ver"],
                                   arch=archs[arch]) for x in p.get("alldeps", []) ])
      kw =  { "package": p["name"],
              "version": p["ver"],
              "arch": archs[arch],
              "dependencies_fmt":
                "".join([
                          format(conf.get("package_format", "%(package)s %(version)s "),
                                 package=x["name"], version=x["ver"], arch=archs[arch])
                          for x in p.get("deps", [])
                 ]),
              "alldependencies_fmt":
                "".join([
                          format(conf.get("package_format", "%(package)s %(version)s "),
                                 package=x["name"], version=x["ver"], arch=archs[arch])
                          for x in p.get("alldeps", [])
                ])
            }

      body = format(conf.get(key, {}).get("body", ""), **kw)
      subj = format(conf.get(key, {}).get("subject", "%(package)s %(version)s: "+key), **kw)

      to = conf.get(key, {}).get("to", "")
      if isinstance(to, dict):
        to = to.get(p["name"], to.get("default", ""))
      if isinstance(to, list):
        to = ",".join(to)
      to = [ x.strip() for x in to.split(",") ] if to else []

      sender = format(conf.get(key, {}).get("from", "noreply@localhost"), **kw)
      if body == "" or not to:
        debug(format("Not sending email notification for %(package)s %(version)s (%(arch)s)",
                     package=p["name"], version=p["ver"], arch=archs[arch]))
        continue
      body = ("Subject: %s\nFrom: %s\nTo: %s\n\n" % (subj, sender, ", ".join(to))) + body
      if dryRun:
        debug(format("Notification email for %(package)s %(version)s (%(arch)s) follows:\n%(body)s",
                     package=p["name"], version=p["ver"], arch=archs[arch], body=body))
      else:
        try:
          mailer.sendmail(sender, to, body)
          debug(format("Sent email notification for %(package)s %(version)s (%(arch)s)",
                       package=p["name"], version=p["ver"], arch=archs[arch]))
        except Exception as e:
          error(format("Cannot send email notification for %(package)s %(version)s (%(arch)s)",
                       package=p["name"], version=p["ver"], arch=archs[arch]))

def hostport(s, defaultPort):
  host = s.split(":", 1)
  try:
    port = len(host) == 2 and int(host[1]) or defaultPort
  except ValueError:
    port = defaultPort
  host = host[0]
  return host,port

def mesos_resolve(host, dns, jget):
  for d in sorted(dns, key=lambda k: random()):
    ips = [ x["ip"] for x in jget(format("http://%(dns)s/v1/hosts/%(host)s", dns=d, host=host))
                    if x.get("ip", None) ]
    if ips:
      return ips
  return [host]  # fallback to input on error

def main():
  parser = ArgumentParser()
  parser.add_argument("action", metavar="sync-cvmfs|sync-dir|sync-alien|sync-rpms|test-rules")
  parser.add_argument("--pkgname", dest="pkgName")
  parser.add_argument("--pkgver", dest="pkgVer")
  parser.add_argument("--pkgarch", dest="pkgArch")
  parser.add_argument("--test-conf", dest="testConf")
  parser.add_argument("--config", "-c", dest="configFile", default="aliPublish.conf",
                      help="Configuration file")
  parser.add_argument("--debug", "-d", dest="debug", action="store_true", default=False,
                      help="Debug output")
  parser.add_argument("--abort-at-start", dest="abort", action="store_true", default=False,
                      help="Abort any pending CVMFS transaction at start")
  parser.add_argument("--no-notification", dest="notify", action="store_false", default=True,
                      help="Do not send any notification (ignore configuration)")
  parser.add_argument("--dry-run", "-n", dest="dryRun", action="store_true", default=False,
                      help="Do not write or publish anything")
  parser.add_argument("--pidfile", "-p", dest="pidFile", default=None,
                      help="Write PID to this file and do not run if already running")
  parser.add_argument("--cache-deps-dir", dest="cacheDepsDir", default=None,
                      help="Directory where to cache package dependencies (optional)")
  parser.add_argument("--override", dest="override", nargs="+",
                      help="Override configuration options in JSON format")
  args = parser.parse_args()

  overrideConf = {}
  try:
    for o in args.override if args.override else {}:
      overrideConf.update(json.loads(o))
  except:
    parser.error("Malformed JSON in --override")

  logger = logging.getLogger()
  loggerHandler = logging.StreamHandler()
  logger.addHandler(loggerHandler)

  loggerHandler.setFormatter(logging.Formatter('%(levelname)-5s: %(message)s'))
  if args.debug: logger.setLevel(logging.DEBUG)
  else: logger.setLevel(logging.INFO)

  logging.getLogger("requests").setLevel(logging.WARNING)
  logging.getLogger("urllib3").setLevel(logging.WARNING)

  progDir = dirname(realpath(__file__))

  try:
    debug(format("Reading configuration from %(configFile)s (current directory: %(curDir)s)",
                 configFile=args.configFile, curDir=getcwd()))
    with open(args.configFile, "r") as cf:
      conf = yaml.safe_load(cf.read())
  except (IOError, YAMLError) as e:
    error(format("While reading %(configFile)s: " + str(e), configFile=args.configFile))
    sys.exit(1)

  if overrideConf:
    debug("Overriding configuration: " + json.dumps(overrideConf, indent=2))
    conf.update(overrideConf)

  if conf is None: conf = {}
  if conf.get("include", None) is None: conf["include"] = {}
  if conf.get("exclude", None) is None: conf["exclude"] = {}
  conf["http_ssl_verify"]   = conf.get("http_ssl_verify"  , True)
  conf["conn_timeout_s"]    = conf.get("conn_timeout_s"   , 6.05)
  conf["conn_retries"]      = conf.get("conn_retries"     , 3)
  conf["conn_dethrottle_s"] = conf.get("conn_dethrottle_s", 0)
  conf["kill_after_s"]      = conf.get("kill_after_s"     , 3600)
  conf["exclude_older_d"]   = conf.get("exclude_older_d"  , 0)  # in days; 0 == don't exclude

  doExit = False

  conf["mesos_dns"] = [ ":".join(map(str, hostport(x, 8123))) for x in conf.get("mesos_dns", []) ]
  shuffle(conf["mesos_dns"])

  # Connection handler
  connParams = dict((k, conf[k]) for k in [ "http_ssl_verify", "conn_timeout_s",
                                            "conn_retries", "conn_dethrottle_s" ])
  connParams["cache_dir"] = args.cacheDepsDir
  jget = JGet(**connParams)

  conf["package_dir"] = conf.get("package_dir", conf.get("cvmfs_package_dir", None))
  conf["modulefile"] = conf.get("modulefile", conf.get("cvmfs_modulefile", None))

  if not isinstance(conf.get("architectures", None), dict):
    error("architectures must be a dict of dicts")
    doExit = True
  if not isinstance(conf.get("base_url", None), basestring):
    error("base_url must be a string")
    doExit = True
  conf["auto_include_deps"] = conf.get("auto_include_deps", True)
  if not isinstance(conf["auto_include_deps"], bool):
    error("auto_include_deps must be a boolean")
    doExit = True
  conf["notification_email"] = conf.get("notification_email", {}) if args.notify else {}
  if not isinstance(conf["notification_email"], dict):
    error("notification_email must be a dict of dicts")
    doExit = True
  if not isinstance(conf["http_ssl_verify"], bool):
    error("http_ssl_verify must be a bool")
    doExit = True

  if doExit: exit(1)

  # Resolve base_url name via Mesos
  us = urlsplit(conf["base_url"])
  if us.netloc.endswith(".mesos") and conf["mesos_dns"] and args.action != "test-rules":
    us = us._replace( netloc=choice(mesos_resolve(us.netloc, conf["mesos_dns"], jget)) )
    conf["base_url"] = urlunsplit(us)

  debug("Configuration: " + json.dumps(conf, indent=2))
  incexc = conf.get("filter_order", "include,exclude")
  if incexc == "include,exclude": includeFirst = True
  elif incexc == "exclude,include": includeFirst = False
  else:
    error("filter_order can be include,exclude or exclude,include")
    sys.exit(1)

  rules = { "include": {}, "exclude": {} }
  for arch,maps in conf["architectures"].iteritems():
    for r in rules.keys():
      rules[r][arch] = isinstance(maps, dict) and maps.get(r, {}) or {}
      for uk in set(conf[r].keys()+rules[r][arch].keys()):

        # Specific (per-arch) rule always wins
        general  = conf[r].get(uk, [])
        specific = rules[r][arch].get(uk, [])

        if isinstance(general, list) and isinstance(specific, list):
          rules[r][arch][uk] = specific + general
        elif not specific and specific != False:
          # specific not specified: general wins
          rules[r][arch][uk] = general
        elif isinstance(specific, bool):
          # specific overrides all (it's a bool)
          rules[r][arch][uk] = specific
        elif isinstance(general, bool):
          # specific overrides all, again (it's a list)
          rules[r][arch][uk] = specific
        else:
          assert False, "Unhandled case: %s rule for %s (%s): general=%s, specific=%s" % (r, uk, arch, general, specific)

  debug("Per architecture include/exclude rules: %s" % json.dumps(rules, indent=2))

  if args.action in [ "sync-cvmfs", "sync-dir", "sync-alien", "sync-rpms" ]:
    chdir("/")
    if args.pidFile:
      try:
        otherPid = int(open(args.pidFile, "r").read().strip())
        kill(otherPid, 0)
        runningFor = time() - getmtime(args.pidFile)
        if runningFor > conf["kill_after_s"]:
          kill(otherPid, 9)
          error("aliPublish with PID %d in overtime (%ds): killed" % (otherPid, runningFor))
          otherPid = 0
      except (IOError, OSError, ValueError):
        otherPid = 0
      if otherPid:
        error("aliPublish already running with PID %d for %ds" % (otherPid, runningFor))
        sys.exit(1)
      try:
        with open(args.pidFile, "w") as f:
          f.write(str(getpid()))
      except IOError as e:
        error("Cannot write pidfile %s, aborting" % args.pidFile)
        sys.exit(1)
    if args.action in [ "sync-cvmfs", "sync-dir" ]:
      if not isinstance(conf["package_dir"], basestring):
        error("[cvmfs_]package_dir must be a string")
        doExit = True
      if not isinstance(conf["modulefile"], basestring):
        error("[cvmfs_]modulefile must be a string")
        doExit = True
    if args.action == "sync-cvmfs":
      if not isinstance(conf.get("cvmfs_repository", None), basestring):
        error("cvmfs_repository must be a string")
        doExit = True
      if doExit: sys.exit(1)
      archKey = "CVMFS"
      pub = CvmfsServer(repository=conf["cvmfs_repository"],
                        modulefileTpl=conf["modulefile"],
                        pkgdirTpl=conf["package_dir"],
                        publishScriptTpl=open(progDir+"/pub-file-template.sh").read(),
                        connParams=connParams,
                        dryRun=args.dryRun)
    elif args.action == "sync-dir":
      if doExit: sys.exit(1)
      archKey = "dir"
      pub = PlainFilesystem(modulefileTpl=conf["modulefile"],
                            pkgdirTpl=conf["package_dir"],
                            publishScriptTpl=open(progDir+"/pub-file-template.sh").read(),
                            connParams=connParams,
                            dryRun=args.dryRun)
    elif args.action == "sync-alien":
      archKey = "AliEn"
      pub = AliEnPackMan(publishScriptTpl=open(progDir+"/pub-alien-template.sh").read(),
                         connParams=connParams,
                         dryRun=args.dryRun)
    else:
      if not isinstance(conf.get("rpm_repo_dir", None), basestring):
        error("rpm_repo_dir must be a string")
        sys.exit(1)
      conf["rpm_updatable"] = conf.get("rpm_updatable", False)
      archKey = "RPM"
      pub = RPM(repoDir=conf["rpm_repo_dir"],
                publishScriptTpl=open(progDir+"/pub-rpms-template.sh").read(),
                connParams=connParams,
                genUpdatableRpms=conf["rpm_updatable"],
                dryRun=args.dryRun)
    if args.abort:
      pub.abort(force=True)

    architectures = dict((arch, maps.get(archKey, arch) if isinstance(maps, dict) else arch)
                         for (arch,maps) in conf["architectures"].iteritems())
    architectures = dict((k,v) for (k,v) in architectures.iteritems() if v)
    debug("Architecture names mappings: %s" % json.dumps(architectures, indent=2))
    r = sync(pub=pub,
             architectures=architectures,
             baseUrl=conf["base_url"],
             rules=rules,
             excludeOlderThanSec=conf["exclude_older_d"] * 86400,
             includeFirst=includeFirst,
             autoIncludeDeps=conf["auto_include_deps"],
             notifEmail=conf["notification_email"],
             dryRun=args.dryRun,
             jget=jget)
    debug("Made %d unique HTTP requests (%d remote requests including retries, %d read from cache)" % \
          (jget.count_req, jget.count_req_retries, jget.count_cached))
    debug("Summary of requested URLs (DIR=direct access, HIT=cache hit, MIS=cache miss):")
    for u in jget.urls:
      debug("[%s] %s" % (u["cached"], u["url"]))
    sys.exit(0 if r else 1)
  elif args.action == "test-rules":
    testRules = {}

    if args.pkgName and args.pkgVer and args.pkgArch:
      # Test rules using command-line arguments
      if args.testConf:
        parser.error("cannot specify at the same time a test file and --pkg* arguments")
      testRules = { args.pkgArch: { args.pkgName: { args.pkgVer: True } } }

    else:
      # Test rules by reading them from a configuration file
      if args.pkgName or args.pkgVer or args.pkgArch:
        parser.error("not all required --pkg* arguments were specified")

      # Do we need to use a default test file?
      if not args.testConf:
        m = search("^aliPublish(|.*)\.conf$", args.configFile)
        if m:
          args.testConf = "test%s.yaml" % m.group(1)
          debug("Using %s as default configuration file for tests" % args.testConf)

      try:
        testRules = yaml.safe_load(open(args.testConf).read())
      except (IOError, YAMLError) as e:
        error("Cannot open rules to test: %s" % e)
        sys.exit(1)

    # At this point we have everything in testRules, let's test them
    for arch in testRules:
      for pkg in testRules[arch]:
        for ver in testRules[arch][pkg]:
          match = applyFilter(ver,
                              0,
                              rules["include"].get(arch, {}).get(pkg, None),
                              rules["exclude"].get(arch, {}).get(pkg, None),
                              includeFirst,
                              0)
          msg = format(match and "%(arch)s: %(pkg)s ver %(ver)s matches filters"
                             or "%(arch)s: %(pkg)s ver %(ver)s does NOT match filters",
                       arch=arch, pkg=pkg, ver=ver)
          if match != testRules[arch][pkg][ver]:
            error(msg + (match and " but it should not" or " but it should"))
            sys.exit(1)
          info(msg)
    info("All rules%s tested with success" % (" in "+args.testConf if args.testConf else ""))
    sys.exit(0)

  else:
    parser.error("wrong action, see --help")

if __name__ == "__main__":
  main()
