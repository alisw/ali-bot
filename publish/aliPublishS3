#!/usr/bin/env python3

import logging, gzip, sys, json, yaml, errno, boto3, requests
import botocore.exceptions
from glob import glob
from argparse import ArgumentParser
from datetime import datetime, timedelta, timezone
from time import time
from logging import debug, info, warning, error
from re import search, escape
from os.path import isdir, isfile, realpath, dirname, getmtime, join, basename, abspath, exists
from os import chmod, remove, getcwd, getpid, kill, makedirs, environ, listdir
from tempfile import NamedTemporaryFile, mkdtemp
from subprocess import Popen, PIPE, STDOUT, DEVNULL, getstatusoutput
from smtplib import SMTP
from xml.etree.ElementTree import ElementTree

def format(s, **kwds):
  return s % kwds

def rmrf(path):
  err, out = getstatusoutput("rm -fr %s" % path)
  if err:
    debug(out)

def searchMany(name, exprs):
  if isinstance(exprs, list):
    return any(search(e, name) for e in exprs)
  return exprs == True

def applyFilter(name, includeRules, excludeRules, includeFirst):
  if includeFirst:
    return searchMany(name, includeRules) and not searchMany(name, excludeRules)
  if searchMany(name, excludeRules):
    return False
  # process exclude first, and no explicit include rule: keep it
  return includeRules is None or searchMany(name, includeRules)

def runInstallScript(script, dryRun, **kwsub):
  if dryRun:
    debug(("Dry run: publish script follows:\n" + script) % kwsub)
    return 0
  with NamedTemporaryFile(delete=False) as fp:
    fn = fp.name
    fp.write((script % kwsub).encode("utf-8"))
  chmod(fn, 0o700)
  debug("Created unpack script: %s", fn)
  rv = execute(fn)
  remove(fn)
  debug("Unpack script %s returned %d", fn, rv)
  return rv

def execute(command):
  popen = Popen(command, shell=False, stdin=DEVNULL, stdout=PIPE, stderr=STDOUT,
                universal_newlines=True)
  for line in iter(popen.stdout.readline, ""):
    debug("Script: %s", line.rstrip("\n"))
  output, _ = popen.communicate()
  for line in output.splitlines():
    debug("Script: %s", line)
  return popen.returncode

def grabOutput(command):
  debug("Executing command: %s", " ".join(command))
  popen = Popen(command, shell=False, stdout=PIPE, stderr=STDOUT)
  out = popen.communicate()[0]
  return (popen.returncode, out.decode("utf-8"))

class PublishException(Exception):
  pass

class PlainFilesystem(object):

  def __init__(self, modulefileTpl, pkgdirTpl, publishScriptTpl,
                     connParams, dryRun=False):

    self._repository         = ""
    self._modulefileTpl      = modulefileTpl
    self._pkgdirTpl          = pkgdirTpl
    self._publishScriptTpl   = publishScriptTpl
    self._connParams         = connParams
    self._dryRun             = dryRun
    self._countChanges       = 0

  def _kw(self, url, arch, pkgName, pkgVer):
    kw =  { "url": url, "package": pkgName, "version": pkgVer, "repo": self._repository or "filesystem",
            "arch": arch }
    kw.update({ "pkgdir": self._pkgdirTpl % kw })
    kw.update({ "modulefile": self._modulefileTpl % kw })
    kw.update(self._connParams)
    kw["http_ssl_verify"] = 1 if kw["http_ssl_verify"] else 0
    return kw

  def installed(self, arch, pkgName, pkgVer):
    kw = self._kw(None, arch, pkgName, pkgVer)
    debug("%(repo)s: checking if %(package)s %(version)s is installed for %(arch)s" % kw)
    return isdir(kw["pkgdir"]) or isfile(kw["modulefile"])

  def install(self, url, arch, pkgName, pkgVer, deps, allDeps):
    kw = self._kw(url, arch, pkgName, pkgVer)
    rv = runInstallScript(self._publishScriptTpl, self._dryRun, **kw)
    if rv == 0:
      self._countChanges += 1
    else:
      self._cleanup(arch, pkgName, pkgVer)
    return rv

  def _cleanup(self, arch, pkgName, pkgVer):
    kw = self._kw(None, arch, pkgName, pkgVer)
    debug("%(repo)s: cleaning up %(pkgdir)s and %(modulefile)s" % kw)
    rmrf(kw["pkgdir"])
    rmrf(kw["modulefile"])

  def transaction(self):
    return True

  def abort(self, force=False):
    return True

  def publish(self, architectures):
    return True


class CvmfsServer(PlainFilesystem):

  def __init__(self, repository, modulefileTpl, pkgdirTpl, publishScriptTpl,
                     connParams, dryRun=False):
    super(CvmfsServer, self).__init__(modulefileTpl, pkgdirTpl,
                                      publishScriptTpl, connParams, dryRun)
    self._inCvmfsTransaction = False
    self._repository         = repository

  def transaction(self):
    if self._inCvmfsTransaction:
      debug("%s: already in a transaction", self._repository)
      return True
    elif self._dryRun:
      info("%s: started transaction (dry run)", self._repository)
      self._inCvmfsTransaction = True
      return True
    else:
      if execute([ "cvmfs_server", "transaction", self._repository ]) == 0:
        info("%s: started transaction", self._repository)
        self._inCvmfsTransaction = True
        return True
      error("%s: cannot commence transaction: maybe another one is in progress?",
            self._repository)
      return False

  def abort(self, force=False):
    if not self._inCvmfsTransaction and not force:
      debug("%s: no transaction to abort", self._repository)
      return True
    if self._dryRun and not force:
      info("%s: transaction aborted (dry run)", self._repository)
      self._inCvmfsTransaction = False
      return True
    rv = execute([ "cvmfs_server", "abort", "-f", self._repository ])
    if rv == 0:
      info("%s: transaction aborted", self._repository)
      self._inCvmfsTransaction = False
      return True
    error("%s: cannot abort transaction", self._repository)
    return False

  def publish(self, architectures):
    if not self._inCvmfsTransaction:
      debug("%s: not in a transaction", self._repository)
      return True
    if not self._countChanges:
      debug("%s: nothing to publish, cancelling transaction", self._repository)
      return self.abort()
    info("%s: publishing transaction, %d new package(s)",
         self._repository, self._countChanges)
    if self._dryRun:
      info("%s: transaction published (dry run)", self._repository)
      return True
    rv = execute([ "cvmfs_server", "publish", self._repository ])
    if rv == 0:
      info("%s: transaction published!", self._repository)
      self._inCvmfsTransaction = False
      return True
    else:
      error("%s: cannot publish CVMFS transaction, aborting", self._repository)
      self.abort()
      return False


class AliEn:

  def __init__(self, connParams, repository, package_dir, dryRun=False):
    self._session = requests.Session()
    self._session.timeout = connParams["conn_timeout_s"]
    # "verify" can be a string pointing to a CA file or directory, or a
    # boolean to switch host cert verification on or off.
    self._session.verify = \
      environ.get("ALIEN_CA_BUNDLE", bool(connParams["http_ssl_verify"]))
    self._session.cert = \
      (environ["ALIEN_CLIENT_CERT"], environ["ALIEN_CLIENT_KEY"])
    self._session.auth = \
      requests.auth.HTTPBasicAuth(environ["ALIEN_USER"], environ["ALIEN_TOKEN"])
    self._dryRun = dryRun
    self._packs = None
    self._cvmfs_package_dir = package_dir
    self._repository = repository

  def _list_packs(self):
    """Cache the full package list from alimonitor and perform sanity checks."""
    if self._packs:
      return self._packs

    debug("AliEn: fetching list of packages from alimonitor")
    response = self._session.get("https://alimonitor.cern.ch/export/packages.jsp")
    if response.status_code != requests.codes.ok:
      error("AliEn: got HTTP %d from alimonitor when fetching package list; "
            "aborting", response.status_code)
      raise PublishException(response)

    try:
      self._packs = response.json()
    except json.JSONDecodeError as exc:
      error("AliEn: could not decode response as JSON", exc_info=exc)
      raise PublishException(response) from exc

    if not self._packs:
      error("AliEn: returned empty package list")
      raise PublishException(response)

    # As a simple sanity check, make sure we get the right order of magnitude
    # in the number of existing packages.
    expect_min_packages = 20000
    if len(self._packs) < expect_min_packages:
      error("AliEn: got fewer packages (%d) than expected (%d)",
            len(self._packs), expect_min_packages)
      raise PublishException(response)

    if any(p["name"] == "AliRoot" for p in self._packs):
      debug("AliEn: AliEn connection and APIs appear to work")
      return self._packs

    error("AliEn: API response incorrect (no AliRoot packages found): %r",
          self._packs)
    raise PublishException(response)

  def installed(self, arch, pkgName, pkgVer):
    debug("AliEn: checking if %s %s is installed for %s", pkgName, pkgVer, arch)
    return any(pkg["name"] == pkgName and pkg["version"] == pkgVer and
               arch in pkg["platforms"] for pkg in self._list_packs())

  def install(self, url, arch, pkgName, pkgVer, deps, allDeps):
    request_data = {
      "name": pkgName,
      "version": pkgVer,
      "platform": arch,
      "dependencies": ",".join(f"VO_ALICE@{dep['name']}::{dep['ver']}"
                               for dep in deps),
      "meta_json_path": join(self._cvmfs_package_dir % {
          "repo": self._repository,
          "arch": arch,
          "package": pkgName,
          "version": pkgVer,
        }, ".meta.json"),
    }
    if self._dryRun:
      debug("Dry run: would have registered %r with alimonitor", request_data)
    else:
      try:
        self._session.get("https://alimonitor.cern.ch/packages/define.jsp",
                          # Set stream=False to read the response body and
                          # release the connection back to the pool.
                          params=request_data, stream=False, timeout=30) \
                     .raise_for_status()
      except requests.HTTPError as exc:
        error("AliEn: failed to register new package", exc_info=exc)
        return 1
    return 0   # simulate successful exit status from script

  def transaction(self):
    return True

  def abort(self, force=False):
    return True

  def publish(self, architectures):
    return True

class RPM(object):

  def __init__(self, publishScriptTpl, connParams, genUpdatableRpms,
               baseUrl, s3Client, s3Bucket, dryRun=False):
    self._dryRun = dryRun
    self._genUpdatableRpms = genUpdatableRpms
    self._stageDir = mkdtemp(prefix="staging-", dir=getcwd())
    self._publishScriptTpl = publishScriptTpl
    self._countChanges = 0
    self._connParams = connParams
    self._inRpmTransaction = False
    self._s3 = s3Client
    self._baseUrl = baseUrl
    self._bucket = s3Bucket
    self._s3_path = "UpdRPMS" if genUpdatableRpms else "RPMS"
    self._remoteRpms = {}

  def _kw(self, url, arch, pkgName, pkgVer, workDir=None, deps=None):
    stageDir = join(self._stageDir, arch)
    kw =  { "url"          : url,
            "package"      : pkgName,
            "version"      : pkgVer,
            "version_rpm"  : pkgVer.replace("-", "_"),
            "arch"         : arch,
            "dependencies" : deps,
            "repodir"      : stageDir,
            "workdir"      : workDir,
            "stagedir"     : stageDir,
            "updatable"    : "1" if self._genUpdatableRpms else "" }
    kw.update(self._connParams)
    kw["http_ssl_verify"] = 1 if kw["http_ssl_verify"] else 0
    kw["rpm"] = ("alisw-%(package)s-%(version_rpm)s-1.%(arch)s.rpm"
                 if self._genUpdatableRpms else
                 "alisw-%(package)s+%(version)s-1-1.%(arch)s.rpm") % kw
    return kw

  def installed(self, arch, pkgName, pkgVer):
    if arch not in self._remoteRpms:
      info("RPM: fetching list of remote RPMs for %s", arch)
      self._remoteRpms[arch] = [
        item["Key"]
        for page in self._s3.get_paginator("list_objects_v2").paginate(
            Bucket=self._bucket, Delimiter="/",
            Prefix="%s/%s/" % (self._s3_path, arch))
        for item in page.get("Contents", ())
        if item["Key"].endswith(".rpm")
      ]

    kw = self._kw(None, arch, pkgName, pkgVer)
    debug("RPM: checking if %(rpm)s exists for %(package)s %(version)s on %(arch)s" % kw)
    s3_key = "{s3_path}/{arch}/{rpm}".format(s3_path=self._s3_path, **kw)
    return s3_key in self._remoteRpms[arch]

  def install(self, url, arch, pkgName, pkgVer, deps, allDeps):
    # All RPMs are created in workDir (temporary, destroyed after creation).
    # Eventually they are moved to stageDirArch (not the repository).

    workDir = "/dryrun_doesnotexist" if self._dryRun else \
      mkdtemp(prefix="aliPublish-RPM-", dir=join(self._stageDir, "tmp"))
    kw = self._kw(url, arch, pkgName, pkgVer, workDir,
                  " ".join(["alisw-%s+%s" % (x["name"], x["ver"]) for x in deps]))
    stageDirArch = kw["stagedir"]

    if not self._dryRun:
      debug("RPM: created temporary working directory %(workdir)s" % kw)
      debug("RPM: creating staging directory %s", stageDirArch)
      try:
        makedirs(stageDirArch)
      except OSError as exc:
        if not isdir(stageDirArch) or exc.errno != errno.EEXIST:
          error("RPM: error creating staging directory %s", stageDirArch)
          rmrf(workDir)
          return 1

    rv = runInstallScript(self._publishScriptTpl, self._dryRun, **kw)
    if rv == 0:
      self._countChanges += 1
    debug("RPM: removing temporary working directory %(workdir)s" % kw)
    rmrf(workDir)
    return rv

  def transaction(self):
    if not self._inRpmTransaction and not self._dryRun:
      self._inRpmTransaction = True
      debug("RPM: cleaning up staging dir %s", self._stageDir)
      rmrf(self._stageDir)
      stagingTmp = join(self._stageDir, "tmp")
      debug("RPM: creating staging tempdir %s", stagingTmp)
      try:
        makedirs(stagingTmp)
      except OSError as exc:
        if not isdir(stagingTmp) or exc.errno != errno.EEXIST:
          error("RPM: error creating staging tempdir %s", stagingTmp)
          return False
    return True

  def abort(self, force=False):
    if self._inRpmTransaction and not self._dryRun:
      debug("RPM: aborting: cleaning up staging dir %s", self._stageDir)
      rmrf(self._stageDir)
      self._inRpmTransaction = False
    return True

  def publish(self, architectures):
    if self._dryRun:
      info("RPM: not updating repository, dry run")
      return True
    info("RPM: updating repository: %d new package(s) in %d architecture(s)",
         self._countChanges, len(architectures))

    for arch in architectures:
      stagedir = join(self._stageDir, arch)
      mergedir = join(self._stageDir, "merged", arch)

      # Get the list of RPMs to upload now, before we download existing ones.
      new_rpms = glob(join(stagedir, "*.rpm"))
      debug("RPM: %d new RPMs to publish for %s", len(new_rpms), arch)

      # 1. Download (up to 10G) RPMs that are not listed in metadata.
      nonindexed_rpms = set()
      repomd_xml = "/".join((self._s3_path, arch, "repodata", "repomd.xml"))
      try:
        repomd_xml_content = self._s3.get_object(Bucket=self._bucket, Key=repomd_xml)["Body"].read()
      except botocore.exceptions.ClientError as exc:
        warning("No existing repomd.xml found at %s", repomd_xml, exc_info=exc)
      else:
        index_name_match = search(rb"repodata/[0-9a-f]+-primary\.xml\.gz", repomd_xml_content)
        if not index_name_match:
          warning("No reference to primary.xml.gz found in repomd.xml")
        else:
          index_key = "/".join((self._s3_path, arch, index_name_match.group(0).decode("utf-8")))
          try:
            index_file = self._s3.get_object(Bucket=self._bucket, Key=index_key)
          except botocore.exceptions.ClientError as exc:
            warning("Index file at %s not found", index_key, exc_info=exc)
          else:
            with gzip.open(index_file["Body"]) as primary_xml:
              index_rpms = {
                installed_pkg.attrib["href"]   # basename only, even though it's called "href"
                for installed_pkg in ElementTree(file=primary_xml).iterfind(
                    "./c:package/c:location",
                    {"c": "http://linux.duke.edu/metadata/common"},
                )
              }
            have_rpms = {
              basename(rpm_file["Key"])
              for page in self._s3.get_paginator("list_objects_v2").paginate(
                Bucket=self._bucket, Delimiter="/",
                Prefix="%s/%s/" % (self._s3_path, arch),
              )
              for rpm_file in page.get("Contents", ())
              if rpm_file["Key"].endswith(".rpm")
            }
            nonindexed_rpms = have_rpms - index_rpms

      if not new_rpms and not nonindexed_rpms:
        debug("RPM: nothing new to publish for %s; skipping", arch)
        continue
      debug("RPM: found %d non-indexed RPMs for %s", len(nonindexed_rpms), arch)

      downloaded_size = 0

      def update_downloaded_size(n_bytes):
        nonlocal downloaded_size
        downloaded_size += n_bytes

      debug("RPM: creating staging directory %s", stagedir)
      try:
        makedirs(stagedir)
      except OSError as exc:
        if not isdir(stagedir) or exc.errno != errno.EEXIST:
          error("RPM: error creating staging directory %s", stagedir)
          continue

      for rpm_name in nonindexed_rpms:
        if exists(join(stagedir, rpm_name)):
          continue
        debug("RPM: downloading non-indexed RPM: %s", rpm_name)
        self._s3.download_file(
          Filename=join(stagedir, rpm_name),
          Bucket=self._bucket,
          Key="/".join((self._s3_path, arch, rpm_name)),
          Callback=update_downloaded_size,
        )
        if downloaded_size > 10 * 1024**3:   # 10 GiB
          debug("RPM: downloaded %.2f GiB, stopping now", downloaded_size / 1024**3)
          break

      # Create a temporary repo with only the new (and unindexed) RPMs.
      baseUrl = self._baseUrl.rstrip("/") + "/" + self._s3_path + "/" + arch
      createrepo = ["createrepo", "--baseurl", baseUrl, stagedir]
      debug("RPM: %s", " ".join(createrepo))

      if execute(createrepo) == 0:
        info("RPM: repository created with new packages for %s", arch)
      else:
        error("RPM: error creating repository for %s", arch)
        return False

      # Merge the temporary repo with the existing metadata.
      mergerepo = ["mergerepo", "--verbose", "--all",
                   "--nogroups", "--noupdateinfo",
                   "--repo", baseUrl,
                   "--repo", "file://" + abspath(stagedir),
                   "--outputdir", mergedir]
      debug("RPM: %s", " ".join(mergerepo))

      if execute(mergerepo) == 0:
        info("RPM: repository merged with existing repo for %s", arch)
      else:
        error("RPM: error merging repositories for %s", arch)
        return False

      # Upload new RPMs.
      debug("RPM: uploading new RPMs and merged metadata to S3")
      for rpm in new_rpms:
        info("RPM: uploading new RPM %s", basename(rpm))
        self._s3.upload_file(Filename=rpm, Bucket=self._bucket,
                             Key=join(self._s3_path, arch, basename(rpm)))
      # Backup repomd.xml separately, since we overwrite it later.
      new_repomd = repomd_xml + "." + datetime.utcnow().isoformat(timespec="seconds")
      info("RPM: creating backup %s", new_repomd)
      self._s3.copy_object(CopySource={"Bucket": self._bucket, "Key": repomd_xml},
                           Bucket=self._bucket, Key=new_repomd)
      # Upload new, merged metadata files.
      new_files = listdir(join(mergedir, "repodata"))
      # Sort repomd.xml last, so that it is only replaced once the files it
      # refers to are in place.
      new_files.sort(key=lambda name: name == "repomd.xml")
      for new_file in new_files:
        info("RPM: uploading new repo metadata: %s", new_file)
        self._s3.upload_file(Bucket=self._bucket,
                             Filename=join(mergedir, "repodata", new_file),
                             Key=join(self._s3_path, arch, "repodata", new_file))
      # Delete old metadata files on S3.
      cutoff = datetime.now(timezone.utc) - timedelta(days=30)
      old_metadata = [
        old_file["Key"]
        for page in self._s3.get_paginator("list_objects_v2").paginate(
          Bucket=self._bucket, Delimiter="/",
          Prefix="%s/%s/repodata/" % (self._s3_path, arch),
        )
        for old_file in page.get("Contents", ())
        if old_file["LastModified"] < cutoff
        and basename(old_file["Key"]) not in new_files
      ]
      info("RPM: deleting %d old repo metadata files", len(old_metadata))
      self._s3.delete_objects(Bucket=self._bucket, Delete={"Objects": [
        {"Key": old_file} for old_file in old_metadata
      ]})

    # Errors here are not fatal.
    debug("RPM: removing staging directory %s", self._stageDir)
    rmrf(self._stageDir)
    return True

def nameVerFromTar(tar, arch, validPacks):
  for pkgName in validPacks:
    vre = format(r"^(%(pack)s)-(.*?)(\.%(arch)s\.tar\.gz)?$", pack=escape(pkgName), arch=arch)
    vm = search(vre, tar)
    if vm:
      return { "name": vm.group(1), "ver": vm.group(2) }
  return None

def prettyPrintPkg(pkg):
  """Return a human-readable representation of the package."""
  return pkg["name"] + " " + pkg["ver"]


def iterUninstalled(publisher, arch, archName, pubPackages):
  """Generate packages that are not installed in the publisher."""
  for pack in pubPackages:
    if publisher.installed(archName, pack["name"], pack["ver"]):
      debug("%s / %s / %s: already installed: skipping",
            arch, pack["name"], pack["ver"])
    else:
      yield pack


def sync(pub, architectures, s3Client, bucket, baseUrl, basePrefix, rules,
         includeFirst, autoIncludeDeps, notifEmail, dryRun, connParams,
         publishLimit):

  # Template URLs
  packNamesPathTpl = "%(arch)s/dist-direct"
  distPathTpl      = "%(arch)s/%(dist)s/%(pack)s/%(pack)s-%(ver)s"
  verPathTpl       = "%(arch)s/dist-direct/%(pack)s"
  getPackUrlTpl    = ("%(baseUrl)s/%(basePrefix)s/%(arch)s/dist-direct/%(pack)s"
                      "/%(pack)s-%(ver)s/%(pack)s-%(ver)s.%(arch)s.tar.gz")
  newPackages = {}

  def listDir(path):
    """Look up the contents of path on S3."""
    debug("Getting directory contents for %s/%s/", basePrefix, path)
    pages = s3Client.get_paginator("list_objects_v2") \
                    .paginate(Bucket=bucket, Delimiter="/",
                              Prefix="%s/%s/" % (basePrefix, path.rstrip("/")))
    for i, page in enumerate(pages):
      debug("Page %d: got %d subdirs and %d files", i,
            len(page.get("CommonPrefixes", ())), len(page.get("Contents", ())))
      # Create a list of directory entries in the format expected below.
      for ditem in page.get("CommonPrefixes", ()):
        yield {"name": basename(ditem["Prefix"].rstrip("/")),
               "type": "directory"}
      for fitem in page.get("Contents", ()):
        yield {"name": basename(fitem["Key"]), "type": "file"}

  # Prepare the list of packages to install
  for arch in architectures:
    newPackages[arch] = []
    debug("Listing all available tarballs for architecture %s", arch)
    tarballs = frozenset(
      basename(item["Key"])
      # Omit Delimiter="/" to get all keys recursively under TARS/$arch/store/.
      # This is much quicker than walking the directory tree, as we want to get
      # all filenames anyway.
      for page in s3Client.get_paginator("list_objects_v2").paginate(
          Bucket=bucket, Prefix="%s/%s/store/" % (basePrefix, arch),
      )
      for item in page.get("Contents", ())
    )
    debug("Found %d tarballs in total for architecture", len(tarballs))

    # Get valid package names for this architecture
    debug("Getting packages for architecture %s", arch)
    distPackages = sorted(
      (p["name"] for p in listDir(packNamesPathTpl % {"arch": arch})
       if p["type"] == "directory"),
      key=len, reverse=True)
    debug("Packages found: %s", ", ".join(distPackages))

    # Packages to publish
    pubPackages = []

    # Get versions for all valid packages and filter them according to the rules
    for pkgName in distPackages:
      if includeFirst and pkgName not in rules["include"][arch]:
        continue
      if not includeFirst and rules["exclude"][arch].get(pkgName) == True:
        continue
      for pkgTar in listDir(format(verPathTpl, arch=arch, pack=pkgName)):
        if pkgTar["type"] != "directory":
          continue
        nameVer = nameVerFromTar(pkgTar["name"], arch, [pkgName])
        if nameVer is None:
          continue
        pkgVer = nameVer["ver"]
        # Here we decide whether to include/exclude it
        if not applyFilter(pkgVer,
                           rules["include"][arch].get(pkgName, None),
                           rules["exclude"][arch].get(pkgName, None),
                           includeFirst):
          debug("%s / %s / %s: excluded", arch, pkgName, pkgVer)
          continue

        if "%s-%s.%s.tar.gz" % (pkgName, pkgVer, arch) not in tarballs:
          debug("%s / %s / %s: excluded because matching tarball not found",
                arch, pkgName, pkgVer)
          continue

        if not autoIncludeDeps:
          # Not automatically including dependencies, add this package only.
          # Not checking for dups because we can't have any
          pubPackages.append({ "name": pkgName, "ver": pkgVer })
          continue

        # At this point we have filtered in the package: let's see its dependencies!
        # Note that a package always depends on itself (list cannot be empty).
        distPath = format(distPathTpl, dist="dist-runtime",
                          arch=arch, pack=pkgName, ver=pkgVer)
        runtimeDeps = list(listDir(distPath))
        if not runtimeDeps:
          error("%s / %s / %s: cannot list dependencies from %s: skipping",
                arch, pkgName, pkgVer, distPath)
          continue
        debug("%s / %s / %s: listing all dependencies under %s",
              arch, pkgName, pkgVer, distPath)
        for depTar in runtimeDeps:
          if depTar["type"] != "file":
            continue
          depNameVer = nameVerFromTar(depTar["name"], arch, distPackages)
          if depNameVer is None:
            continue
          depName = depNameVer["name"]
          depVer = depNameVer["ver"]
          # Append only if it does not exist yet
          if not any(p["name"] == depName and p["ver"] == depVer
                     for p in pubPackages):
            debug("%s / %s / %s: adding %s %s to publish",
                  arch, pkgName, pkgVer, depName, depVer)
            pubPackages.append({"name": depName, "ver": depVer})

    pubPackages.sort(key=lambda itm: itm["name"])
    debug("%s: %d package(s) candidate for publication: %s",
          arch, len(pubPackages), ", ".join(map(prettyPrintPkg, pubPackages)))

    pubPackages = list(iterUninstalled(pub, arch, architectures[arch], pubPackages))
    debug("%s: %d package(s) will be published: %s",
          arch, len(pubPackages), ", ".join(map(prettyPrintPkg, pubPackages)))

    if publishLimit and len(pubPackages) > publishLimit:
      error("Too many packages to publish (%d); limiting to %d to avoid "
            "running out of temporary disk space",
            len(pubPackages), publishLimit)
      pubPackages = pubPackages[:publishLimit]

    # Packages installation
    for pack in pubPackages:
      # Get direct and indirect dependencies
      deps = {}
      depFail = False
      for key in ("dist", "dist-direct", "dist-runtime"):
        jdeps = list(listDir(format(distPathTpl, dist=key, arch=arch,
                                    pack=pack["name"], ver=pack["ver"])))
        if not jdeps:
          error("%s / %s / %s: cannot get %s dependencies: skipping",
                arch, pack["name"], pack["ver"], key)
          newPackages[arch].append({
            "name": pack["name"], "ver": pack["ver"], "success": False})
          depFail = True
          break
        deps[key] = [nameVerFromTar(x["name"], arch, distPackages)
                     for x in jdeps if x["type"] == "file"]
        deps[key] = [x for x in deps[key]
                     if x is not None and x["name"] != pack["name"]]
      if depFail:
        continue
      # dist-direct-runtime: all entries in dist-direct also in dist-runtime
      deps["dist-direct-runtime"] = [
        x for x in deps["dist-direct"]
        if any(x["name"] == y["name"] for y in deps["dist-runtime"])
      ]

      symlinkUrl = format(getPackUrlTpl,
                          baseUrl=baseUrl.rstrip("/"),
                          basePrefix=basePrefix, arch=arch,
                          pack=pack["name"], ver=pack["ver"])
      pkgUrl = join(baseUrl,
                    requests.get(symlinkUrl,
                                 verify=connParams["http_ssl_verify"],
                                 timeout=connParams["conn_timeout_s"])
                            .text.rstrip().lstrip("./"))  # strip leading "../"

      # Here we can attempt the installation
      def prettyPrintPkgs(key):
        return ", ".join(map(prettyPrintPkg, deps[key]))
      info("%s / %s / %s: getting and installing", arch, pack["name"], pack["ver"])
      info(" * Source: %s", pkgUrl)
      info(" * Direct deps: %s", prettyPrintPkgs("dist-direct"))
      info(" * All deps: %s", prettyPrintPkgs("dist"))
      info(" * Direct runtime deps: %s", prettyPrintPkgs("dist-direct-runtime"))
      info(" * Runtime deps: %s", prettyPrintPkgs("dist-runtime"))

      if not pub.transaction():
        sys.exit(2)  # fatal
      rv = pub.install(pkgUrl, architectures[arch], pack["name"], pack["ver"],
                       deps["dist-direct-runtime"], deps["dist-runtime"])
      newPackages[arch].append({
        "name": pack["name"],
        "ver": pack["ver"],
        "success": rv == 0,
        "deps": deps["dist-direct-runtime"],
        "alldeps": deps["dist-runtime"],
      })
      if rv == 0:
        info("%s / %s / %s: installed successfully",
             arch, pack["name"], pack["ver"])
      else:
        error("%s / %s / %s: publish script failed with %d",
              arch, pack["name"], pack["ver"], rv)

  # Publish eventually
  if pub.publish(architectures.values()):
    totSuccess = 0
    totFail = 0
    for arch, packStatus in newPackages.items():
      nSuccess = sum(1 for x in packStatus if x["success"])
      nFail = len(packStatus) - nSuccess
      totSuccess += nSuccess
      totFail += nFail
      info("%s: install OK for %d/%d package(s): %s", arch, nSuccess, len(packStatus),
           ", ".join(prettyPrintPkg(x) for x in packStatus if x["success"]))
      if nFail:
        error("%s: install failed for %d/%d package(s): %s", arch, nFail, len(packStatus),
              ", ".join(prettyPrintPkg(x) for x in packStatus if not x["success"]))
    if notifEmail:
      notify(notifEmail, architectures, newPackages, dryRun)
    else:
      debug("No email notification configured")
    return totFail == 0 or totSuccess > 0

  return False

def notify(conf, archs, pack, dryRun):
  if not "server" in conf:
    return
  try:
    mailer = SMTP(conf["server"], conf.get("port", 25))
  except Exception as e:
    error("Email notification: cannot connect to %s", conf["server"])
    return
  for arch, packs in pack.items():
    for p in packs:
      key = "success" if p["success"] else "failure"
      deps_fmt = "".join(format(conf.get("package_format", "%(package)s %(version)s "),
                                package=x["name"],
                                version=x["ver"],
                                arch=archs[arch]) for x in p.get("alldeps", []))
      kw = {
        "package": p["name"],
        "version": p["ver"],
        "arch": archs[arch],
        "dependencies_fmt": "".join(
          format(conf.get("package_format", "%(package)s %(version)s "),
                 package=x["name"], version=x["ver"], arch=archs[arch])
          for x in p.get("deps", [])),
        "alldependencies_fmt": "".join(
          format(conf.get("package_format", "%(package)s %(version)s "),
                 package=x["name"], version=x["ver"], arch=archs[arch])
          for x in p.get("alldeps", [])),
      }

      body = conf.get(key, {}).get("body", "") % kw
      subj = conf.get(key, {}).get("subject", "%(package)s %(version)s: " + key) % kw

      to = conf.get(key, {}).get("to", "")
      if isinstance(to, dict):
        to = to.get(p["name"], to.get("default", ""))
      if isinstance(to, list):
        to = ",".join(to)
      to = [x.strip() for x in to.split(",")] if to else []

      sender = conf.get(key, {}).get("from", "noreply@localhost") % kw
      if not body or not to:
        debug("Not sending email notification for %s %s (%s)",
              p["name"], p["ver"], archs[arch])
        continue
      body = ("Subject: %s\nFrom: %s\nTo: %s\n\n%s" %
              (subj, sender, ", ".join(to), body))
      if dryRun:
        debug("Notification email for %s %s (%s) follows:\n%s",
              p["name"], p["ver"], archs[arch], body)
        continue
      try:
        mailer.sendmail(sender, to, body)
      except Exception as e:
        error("Cannot send email notification for %s %s (%s): %s",
              p["name"], p["ver"], archs[arch], e)
        continue
      debug("Sent email notification for %s %s (%s)",
            p["name"], p["ver"], archs[arch])


def main():
  parser = ArgumentParser()
  parser.add_argument("action", metavar="sync-cvmfs|sync-dir|sync-alien|sync-rpms|test-rules")
  parser.add_argument("--pkgname", dest="pkgName")
  parser.add_argument("--pkgver", dest="pkgVer")
  parser.add_argument("--pkgarch", dest="pkgArch")
  # Backward compatibility
  parser.add_argument("--cache-deps-dir", dest="ignoredOption1", default="ignored")
  parser.add_argument("--test-conf", dest="testConf")
  parser.add_argument("--config", "-c", dest="configFile", default="aliPublish.conf",
                      help="Configuration file")
  parser.add_argument("--debug", "-d", dest="debug", action="store_true", default=False,
                      help="Debug output")
  parser.add_argument("--abort-at-start", dest="abort", action="store_true", default=False,
                      help="Abort any pending CVMFS transaction at start")
  parser.add_argument("--no-notification", dest="notify", action="store_false", default=True,
                      help="Do not send any notification (ignore configuration)")
  parser.add_argument("--dry-run", "-n", dest="dryRun", action="store_true", default=False,
                      help="Do not write or publish anything")
  parser.add_argument("--pidfile", "-p", dest="pidFile", default=None,
                      help="Write PID to this file and do not run if already running")
  parser.add_argument("--override", dest="override", nargs="+",
                      help="Override configuration options in JSON format")
  args = parser.parse_args()

  overrideConf = {}
  try:
    for o in args.override if args.override else {}:
      overrideConf.update(json.loads(o))
  except:
    parser.error("Malformed JSON in --override")

  logger = logging.getLogger()
  loggerHandler = logging.StreamHandler()
  logger.addHandler(loggerHandler)
  loggerHandler.setFormatter(logging.Formatter("%(asctime)s %(levelname)-5s: %(message)s"))
  logger.setLevel(logging.DEBUG if args.debug else logging.INFO)
  logging.getLogger("boto3").setLevel(logging.WARNING)
  logging.getLogger("botocore").setLevel(logging.WARNING)
  logging.getLogger("urllib3").setLevel(logging.WARNING)
  logging.getLogger("s3transfer").setLevel(logging.WARNING)

  progDir = dirname(realpath(__file__))

  try:
    debug("Reading configuration from %s (current directory: %s)",
          args.configFile, getcwd())
    with open(args.configFile, "r") as cf:
      conf = yaml.safe_load(cf.read())
  except (IOError, yaml.YAMLError) as e:
    error("While reading %s: %s", args.configFile, e)
    return 1

  if overrideConf:
    debug("Overriding configuration: %s", json.dumps(overrideConf, indent=2))
    conf.update(overrideConf)

  if conf is None: conf = {}
  if conf.get("include", None) is None: conf["include"] = {}
  if conf.get("exclude", None) is None: conf["exclude"] = {}
  conf.setdefault("http_ssl_verify", True)
  conf.setdefault("conn_timeout_s", 6.05)
  conf.setdefault("conn_retries", 3)
  conf.setdefault("conn_dethrottle_s", 0)
  conf.setdefault("kill_after_s", 3600)

  doExit = False

  # Connection handler
  connParams = {k: conf[k] for k in ("http_ssl_verify", "conn_timeout_s",
                                     "conn_retries", "conn_dethrottle_s")}

  conf.setdefault("package_dir", conf.get("cvmfs_package_dir", None))
  conf.setdefault("modulefile", conf.get("cvmfs_modulefile", None))

  if not isinstance(conf.get("architectures", None), dict):
    error("architectures must be a dict of dicts")
    doExit = True
  conf["auto_include_deps"] = conf.get("auto_include_deps", True)
  if not isinstance(conf["auto_include_deps"], bool):
    error("auto_include_deps must be a boolean")
    doExit = True
  conf["notification_email"] = conf.get("notification_email", {}) if args.notify else {}
  if not isinstance(conf["notification_email"], dict):
    error("notification_email must be a dict of dicts")
    doExit = True
  if not isinstance(conf["http_ssl_verify"], bool):
    error("http_ssl_verify must be a bool")
    doExit = True
  conf.setdefault("publish_max_packages", 0)  # 0 == unlimited
  if not isinstance(conf["publish_max_packages"], int):
    error("publish_max_packages must be an integer")
    doExit = True
  if conf["publish_max_packages"] < 0:
    error("publish_max_packages must not be negative; use 0 for unlimited")
    doExit = True

  if doExit: exit(1)

  debug("Configuration: %s", json.dumps(conf, indent=2))
  incexc = conf.get("filter_order", "include,exclude")
  if incexc == "include,exclude": includeFirst = True
  elif incexc == "exclude,include": includeFirst = False
  else:
    error("filter_order can be include,exclude or exclude,include")
    return 1

  rules = { "include": {}, "exclude": {} }
  for arch,maps in conf["architectures"].items():
    for r in rules.keys():
      rules[r][arch] = isinstance(maps, dict) and maps.get(r, {}) or {}
      for uk in set(conf[r].keys()) | set(rules[r][arch].keys()):

        # Specific (per-arch) rule always wins
        general  = conf[r].get(uk, [])
        specific = rules[r][arch].get(uk, [])

        if isinstance(general, list) and isinstance(specific, list):
          rules[r][arch][uk] = specific + general
        elif not specific and specific != False:
          # specific not specified: general wins
          rules[r][arch][uk] = general
        elif isinstance(specific, bool):
          # specific overrides all (it's a bool)
          rules[r][arch][uk] = specific
        elif isinstance(general, bool):
          # specific overrides all, again (it's a list)
          rules[r][arch][uk] = specific
        else:
          assert False, "Unhandled case: %s rule for %s (%s): general=%s, specific=%s" % (r, uk, arch, general, specific)

  debug("Per architecture include/exclude rules: %s", json.dumps(rules, indent=2))

  if args.action in [ "sync-cvmfs", "sync-dir", "sync-alien", "sync-rpms" ]:
    if args.pidFile:
      try:
        otherPid = int(open(args.pidFile, "r").read().strip())
        kill(otherPid, 0)
        runningFor = time() - getmtime(args.pidFile)
        if runningFor > conf["kill_after_s"]:
          kill(otherPid, 9)
          error("aliPublish with PID %d in overtime (%ds): killed", otherPid, runningFor)
          otherPid = 0
      except (IOError, OSError, ValueError):
        otherPid = runningFor = 0
      if otherPid:
        error("aliPublish already running with PID %d for %ds", otherPid, runningFor)
        return 1
      try:
        with open(args.pidFile, "w") as f:
          f.write(str(getpid()))
      except IOError as e:
        error("Cannot write pidfile %s, aborting", args.pidFile)
        return 1
    if args.action in ("sync-cvmfs", "sync-dir", "sync-alien"):
      if not isinstance(conf["package_dir"], str):
        error("[cvmfs_]package_dir must be a string")
        doExit = True
    if args.action in ("sync-cvmfs", "sync-dir"):
      if not isinstance(conf["modulefile"], str):
        error("[cvmfs_]modulefile must be a string")
        doExit = True
    if args.action in ("sync-cvmfs", "sync-alien"):
      if not isinstance(conf.get("cvmfs_repository", None), str):
        error("cvmfs_repository must be a string")
        doExit = True
    if doExit:
      return 1

    s3Client = boto3.client("s3", endpoint_url=conf["s3_endpoint_url"],
                            aws_access_key_id=environ["AWS_ACCESS_KEY_ID"],
                            aws_secret_access_key=environ["AWS_SECRET_ACCESS_KEY"])

    if args.action == "sync-cvmfs":
      archKey = "CVMFS"
      pub = CvmfsServer(repository=conf["cvmfs_repository"],
                        modulefileTpl=conf["modulefile"],
                        pkgdirTpl=conf["package_dir"],
                        publishScriptTpl=open(progDir+"/pub-file-template.sh").read(),
                        connParams=connParams,
                        dryRun=args.dryRun)
    elif args.action == "sync-dir":
      archKey = "dir"
      pub = PlainFilesystem(modulefileTpl=conf["modulefile"],
                            pkgdirTpl=conf["package_dir"],
                            publishScriptTpl=open(progDir+"/pub-file-template.sh").read(),
                            connParams=connParams,
                            dryRun=args.dryRun)
    elif args.action == "sync-alien":
      archKey = "AliEn"
      pub = AliEn(connParams=connParams,
                  repository=conf["cvmfs_repository"],
                  package_dir=conf["package_dir"],
                  dryRun=args.dryRun)
    else:
      conf["rpm_updatable"] = conf.get("rpm_updatable", False)
      archKey = "RPM"
      template = "pub-updatable-rpms-template.sh" if conf["rpm_updatable"] \
        else "pub-rpms-template.sh"
      pub = RPM(publishScriptTpl=open(progDir + "/" + template).read(),
                connParams=connParams,
                genUpdatableRpms=conf["rpm_updatable"],
                baseUrl=conf["base_url"],
                s3Client=s3Client,
                s3Bucket=conf["s3_bucket"],
                dryRun=args.dryRun)
    if args.abort:
      pub.abort(force=True)

    architectures = {arch: maps.get(archKey, arch) if isinstance(maps, dict) else arch
                     for arch, maps in conf["architectures"].items()}
    architectures = {k: v for k, v in architectures.items() if v}
    debug("Architecture names mappings: %s", json.dumps(architectures, indent=2))
    return int(not sync(pub=pub,
                        architectures=architectures,
                        s3Client=s3Client,
                        bucket=conf["s3_bucket"],
                        baseUrl=conf["base_url"],
                        basePrefix=conf["base_prefix"],
                        rules=rules,
                        includeFirst=includeFirst,
                        autoIncludeDeps=conf["auto_include_deps"],
                        notifEmail=conf["notification_email"],
                        dryRun=args.dryRun,
                        connParams=connParams,
                        publishLimit=conf["publish_max_packages"]))
  if args.action == "test-rules":
    testRules = {}

    if args.pkgName and args.pkgVer and args.pkgArch:
      # Test rules using command-line arguments
      if args.testConf:
        parser.error("cannot specify at the same time a test file and --pkg* arguments")
      testRules = {args.pkgArch: {args.pkgName: {args.pkgVer: True}}}

    else:
      # Test rules by reading them from a configuration file
      if args.pkgName or args.pkgVer or args.pkgArch:
        parser.error("not all required --pkg* arguments were specified")

      # Do we need to use a default test file?
      if not args.testConf:
        m = search(r"^aliPublish(|.*)\.conf$", args.configFile)
        if m:
          args.testConf = "test%s.yaml" % m.group(1)
          debug("Using %s as default configuration file for tests", args.testConf)

      try:
        testRules = yaml.safe_load(open(args.testConf).read())
      except (IOError, yaml.YAMLError) as e:
        error("Cannot open rules to test: %s", e)
        return 1

    # At this point we have everything in testRules, let's test them
    for arch in testRules:
      for pkg in testRules[arch]:
        for ver in testRules[arch][pkg]:
          match = applyFilter(ver,
                              rules["include"].get(arch, {}).get(pkg, None),
                              rules["exclude"].get(arch, {}).get(pkg, None),
                              includeFirst)
          msg = ("%s: %s ver %s matches filters%s" if match else
                 "%s: %s ver %s does NOT match filters%s")
          if match != testRules[arch][pkg][ver]:
            error(msg, arch, pkg, ver,
                  " but it should not" if match else " but it should")
            return 1
          info(msg, arch, pkg, ver, "")
    info("All rules%s tested with success",
         " in "+args.testConf if args.testConf else "")
    return 0

  else:
    parser.error("wrong action, see --help")

if __name__ == "__main__":
  sys.exit(main())
